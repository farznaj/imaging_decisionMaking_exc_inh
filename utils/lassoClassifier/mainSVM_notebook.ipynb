{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6eee2dc9-1ec9-4705-aa55-ecd06847a05d"
    }
   },
   "source": [
    "## Specify variables for the analysis: \n",
    "    - Data (mouse, day, sessions)\n",
    "    - Neuron type: excit, inhibit, or all\n",
    "    - Current-choice or previous-choice SVM training\n",
    "        if current-choice, specify epoch of training, the outcome (corr, incorr, all) and strength (easy, medium, hard, all) of trials for training SVM.\n",
    "        if previous-choice, specify ITI flag\n",
    "    - Trials that will be used for projections and class accuracy traces (corr, incorr, all, trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the option to toggle on/off the raw code. Copied from http://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer\n",
    "import sys\n",
    "if 'ipykernel' in sys.modules:\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    HTML('''<script>\n",
    "    code_show=true; \n",
    "    function code_toggle() {\n",
    "     if (code_show){\n",
    "     $('div.input').hide();\n",
    "     } else {\n",
    "     $('div.input').show();\n",
    "     }\n",
    "     code_show = !code_show\n",
    "    } \n",
    "    $( document ).ready(code_toggle);\n",
    "    </script>\n",
    "    <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Only run the following section if you are running the code in jupyter, not if on the cluster or in spyder!\n",
    "if ('ipykernel' in sys.modules and ~any('SPYDER' in name for name in os.environ))==-1:\n",
    "    \n",
    "    # Set these variables:\n",
    "    mousename = 'fni17'\n",
    "    imagingFolder = '151102'\n",
    "    mdfFileNumber = [1,2] \n",
    "\n",
    "    roundi = 2; # For the same dataset we run the code multiple times, each time we select a random subset of neurons (of size n, n=.95*numTrials)\n",
    "    trialHistAnalysis = 1;    \n",
    "\n",
    "    iTiFlg = 2; # Only needed if trialHistAnalysis=1; short ITI, 1: long ITI, 2: all ITIs.\n",
    "    numSamples = 2 #100; # number of iterations for finding the best c (inverse of regularization parameter)\n",
    "    neuronType = 2; # 0: excitatory, 1: inhibitory, 2: all types.    \n",
    "    saveResults = 0; # save results in mat file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # The following vars don't usually need to be changed\n",
    "    setNsExcluded = 0; # if 1, NsExcluded will be set even if it is already saved.\n",
    "    doPlots = 1; # Whether to make plots or not.\n",
    "    saveHTML = 0; # whether to save the html file of notebook with all figures or not.\n",
    "\n",
    "    if trialHistAnalysis==1: # more parameters are specified in popClassifier_trialHistory.m\n",
    "    #        iTiFlg = 1; # 0: short ITI, 1: long ITI, 2: all ITIs.\n",
    "        epEnd_rel2stimon_fr = 0 # 3; # -2 # epEnd = eventI + epEnd_rel2stimon_fr\n",
    "    else:\n",
    "        # not needed to set ep_ms here, later you define it as [choiceTime-300 choiceTime]ms # we also go 30ms back to make sure we are not right on the choice time!\n",
    "        ep_ms = [809, 1109] #[425, 725] # optional, it will be set according to min choice time if not provided.# training epoch relative to stimOnset % we want to decode animal's upcoming choice by traninig SVM for neural average responses during ep ms after stimulus onset. [1000, 1300]; #[700, 900]; # [500, 700]; \n",
    "        # outcome2ana will be used if trialHistAnalysis is 0. When it is 1, by default we are analyzing past correct trials. If you want to change that, set it in the matlab code.\n",
    "        outcome2ana = 'corr' # '', corr', 'incorr' # trials to use for SVM training (all, correct or incorrect trials)\n",
    "        strength2ana = 'all' # 'all', easy', 'medium', 'hard' % What stim strength to use for training?\n",
    "        thStimStrength = 3; # 2; # threshold of stim strength for defining hard, medium and easy trials.\n",
    "        th_stim_dur = 800; # min stim duration to include a trial in timeStimOnset\n",
    "\n",
    "    trs4project = 'trained' # 'trained', 'all', 'corr', 'incorr' # trials that will be used for projections and the class accuracy trace; if 'trained', same trials that were used for SVM training will be used. \"corr\" and \"incorr\" refer to current trial's outcome, so they don't mean much if trialHistAnalysis=1. \n",
    "    windowAvgFlg = 1 # if 0, data points during ep wont be averaged when setting X (for SVM training), instead each frame of ep will be treated as a separate datapoint. It helps with increasing number of datapoints, but will make data mor enoisy.\n",
    "\n",
    "    thAct = 5e-4 #5e-4; # 1e-5 # neurons whose average activity during ep is less than thAct will be called non-active and will be excluded.\n",
    "    thTrsWithSpike = 1; # 3 % remove neurons that are active in <thSpTr trials.\n",
    "\n",
    "    pnev2load = [] #[] [3] # which pnev file to load: indicates index of date-sorted files: use 0 for latest. Set [] to load the latest one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing fni17_151102_[1, 2]\n",
      "trialHistAnalysis = 1\n",
      "Analyzing all neurons\n",
      "Analyzing all ITIs\n",
      "windowAvgFlg = 1\n",
      "numSamples = 2\n"
     ]
    }
   ],
   "source": [
    "if neuronType==0:\n",
    "    ntName = 'excit'\n",
    "elif neuronType==1:\n",
    "    ntName = 'inhibit'\n",
    "elif neuronType==2:\n",
    "    ntName = 'all'     \n",
    "\n",
    "if trialHistAnalysis==1:    \n",
    "    if iTiFlg==0:\n",
    "        itiName = 'short'\n",
    "    elif iTiFlg==1:\n",
    "        itiName = 'long'\n",
    "    elif iTiFlg==2:\n",
    "        itiName = 'all'        \n",
    "    \n",
    "print 'Analyzing %s' %(mousename+'_'+imagingFolder+'_'+str(mdfFileNumber)) \n",
    "if trialHistAnalysis==0:\n",
    "    print 'Training %s trials of strength %s. Making projections for %s trials' %(outcome2ana, strength2ana, trs4project)\n",
    "print 'trialHistAnalysis = %i' %(trialHistAnalysis)\n",
    "print 'Analyzing %s neurons' %(ntName)\n",
    "if trialHistAnalysis==1:\n",
    "    print 'Analyzing %s ITIs' %(itiName)\n",
    "elif 'ep_ms' in locals():\n",
    "    print 'training window: [%d %d] ms' %(ep_ms[0], ep_ms[1])\n",
    "print 'windowAvgFlg = %i' %(windowAvgFlg)\n",
    "print 'numSamples = %i' %(numSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d38a63b9-fec1-4f66-856b-d76627310d4c"
    }
   },
   "source": [
    "## Import Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "15730e76-0cf7-445e-9d2d-adb7a6728a60"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import scipy as sci\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import sys\n",
    "from crossValidateModel import crossValidateModel\n",
    "from linearSVM import linearSVM\n",
    "from compiler.ast import flatten\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "if 'ipykernel' in sys.modules and doPlots:\n",
    "    %matplotlib inline  \n",
    "    %config InlineBackend.figure_format = 'svg'\n",
    "matplotlib.rcParams['figure.figsize'] = (6,4) #(8,5)\n",
    "from IPython.display import display\n",
    "import sklearn.svm as svm\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# print sys.path\n",
    "sys.path.append('/home/farznaj/Documents/trial_history/imaging') # Gamal's dir needs to be added using \"if\" that takes the value of pwd\n",
    "# print sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extend the built in two tailed ttest function to one-tailed\n",
    "def ttest2(a, b, **tailOption):\n",
    "    import scipy.stats as stats\n",
    "    import numpy as np\n",
    "    h, p = stats.ttest_ind(a, b)\n",
    "    d = np.mean(a)-np.mean(b)\n",
    "    if tailOption.get('tail'):\n",
    "        tail = tailOption.get('tail').lower()\n",
    "        if tail == 'right':\n",
    "            p = p/2.*(d>0) + (1-p/2.)*(d<0)\n",
    "        elif tail == 'left':\n",
    "            p = (1-p/2.)*(d>0) + p/2.*(d<0)\n",
    "    if d==0:\n",
    "        p = 1;\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Aug 24 15:59:12 2016\n",
    "\n",
    "@author: farznaj\n",
    "\n",
    "This is Farzaneh's first Python code :-) She is very happy and pleased about it :D\n",
    "\n",
    "example call:\n",
    "\n",
    "mousename = 'fni17'\n",
    "imagingFolder = '151021'\n",
    "mdfFileNumber = [1] #(1,2)\n",
    "\n",
    "# optional inputs:\n",
    "postNProvided = 1; # Default:0; If your directory does not contain pnevFile and instead it contains postFile, set this to 1 to get pnevFileName\n",
    "signalCh = [2] # since gcamp is channel 2, should be 2.\n",
    "pnev2load = [] # which pnev file to load: indicates index of date-sorted files: use 0 for latest. Set [] to load the latest one.\n",
    "\n",
    "from setImagingAnalysisNamesP import *\n",
    "\n",
    "imfilename, pnevFileName = setImagingAnalysisNamesP(mousename, imagingFolder, mdfFileNumber, signalCh=signalCh, pnev2load=pnev2load, postNProvided=postNProvided)\n",
    "\n",
    "imfilename, pnevFileName = setImagingAnalysisNamesP(mousename, imagingFolder, mdfFileNumber)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#%%\n",
    "def setImagingAnalysisNamesP(mousename, imagingFolder, mdfFileNumber, **options):\n",
    "\n",
    "    if options.get('signalCh'):\n",
    "        signalCh = options.get('signalCh');    \n",
    "    else:\n",
    "        signalCh = []\n",
    "        \n",
    "    if options.get('pnev2load'):\n",
    "        pnev2load = options.get('pnev2load');    \n",
    "    else:\n",
    "        pnev2load = []\n",
    "        \n",
    "    if options.get('postNProvided'):\n",
    "        postNProvided = options.get('postNProvided');    \n",
    "    else:\n",
    "        postNProvided = 0\n",
    "        \n",
    "    #%%\n",
    "    import numpy as np\n",
    "    import platform\n",
    "    import glob\n",
    "    import os.path\n",
    "        \n",
    "    if len(pnev2load)==0:\n",
    "        pnev2load = [0];\n",
    "            \n",
    "    #%%\n",
    "    dataPath = []\n",
    "    if platform.system()=='Linux':\n",
    "        if os.getcwd().find('sonas')==1: # server\n",
    "            dataPath = '/sonas-hs/churchland/nlsas/data/data/'\n",
    "        else: # office linux\n",
    "            dataPath = '/home/farznaj/Shares/Churchland/data/'\n",
    "    else:\n",
    "        dataPath = '/Users/gamalamin/git_local_repository/Farzaneh/data/'\n",
    "        \n",
    "    #%%        \n",
    "    tifFold = os.path.join(dataPath+mousename,'imaging',imagingFolder)\n",
    "    r = '%03d-'*len(mdfFileNumber)\n",
    "    r = r[:-1]\n",
    "    rr = r % (tuple(mdfFileNumber))\n",
    "    \n",
    "    date_major = imagingFolder+'_'+rr\n",
    "    imfilename = os.path.join(tifFold,date_major+'.mat')\n",
    "    \n",
    "    #%%\n",
    "    if len(signalCh)>0:\n",
    "        if postNProvided:\n",
    "            pnevFileName = 'post_'+date_major+'_ch'+str(signalCh)+'-Pnev*'\n",
    "        else:\n",
    "            pnevFileName = date_major+'_ch'+str(signalCh)+'-Pnev*'\n",
    "            \n",
    "        pnevFileName = glob.glob(os.path.join(tifFold,pnevFileName))   \n",
    "        # sort pnevFileNames by date (descending)\n",
    "        pnevFileName = sorted(pnevFileName, key=os.path.getmtime)\n",
    "        pnevFileName = pnevFileName[::-1]\n",
    "        '''\n",
    "        array = []\n",
    "        for idx in range(0, len(pnevFileName)):\n",
    "            array.append(os.path.getmtime(pnevFileName[idx]))\n",
    "\n",
    "        inds = np.argsort(array)\n",
    "        inds = inds[::-1]\n",
    "        pnev2load = inds[pnev2load]\n",
    "        '''    \n",
    "        if len(pnevFileName)==0:\n",
    "            c = (\"No Pnev file was found\"); print(\"%s\\n\" % c)\n",
    "            pnevFileName = ''\n",
    "        else:\n",
    "            pnevFileName = pnevFileName[pnev2load[0]]\n",
    "            if postNProvided:\n",
    "                p = os.path.basename(pnevFileName)[5:]\n",
    "                pnevFileName = os.path.join(tifFold,p)\n",
    "    else:\n",
    "        pnevFileName = ''\n",
    "    \n",
    "    #%%\n",
    "    return imfilename, pnevFileName\n",
    "    \n",
    "    \n",
    "#%%\n",
    "#imfilename, pnevFileName = setImagingAnalysisNamesP(mousename, imagingFolder, mdfFileNumber, signalCh, pnev2load)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set mat-file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "65954c29-0a9e-47e0-bdd1-fd582bc68831"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/farznaj/Shares/Churchland/data/fni17/imaging/151102/151102_001-002.mat\n",
      "/home/farznaj/Shares/Churchland/data/fni17/imaging/151102/151102_001-002_ch2-PnevPanResults-161004-115936.mat\n",
      "/home/farznaj/Shares/Churchland/data/fni17/imaging/151102/post_151102_001-002_ch2-PnevPanResults-161004-115936.mat\n",
      "/home/farznaj/Shares/Churchland/data/fni17/imaging/151102/more_151102_001-002_ch2-PnevPanResults-161004-115936.mat\n"
     ]
    }
   ],
   "source": [
    "pnev2load = [] #[] [3] # which pnev file to load: indicates index of date-sorted files: use 0 for latest. Set [] to load the latest one.\n",
    "signalCh = [2] # since gcamp is channel 2, should be always 2.\n",
    "postNProvided = 1; # If your directory does not contain pnevFile and instead it contains postFile, set this to 1 to get pnevFileName\n",
    "\n",
    "# from setImagingAnalysisNamesP import *\n",
    "\n",
    "imfilename, pnevFileName = setImagingAnalysisNamesP(mousename, imagingFolder, mdfFileNumber, signalCh=signalCh, pnev2load=pnev2load, postNProvided=postNProvided)\n",
    "\n",
    "postName = os.path.join(os.path.dirname(pnevFileName), 'post_'+os.path.basename(pnevFileName))\n",
    "moreName = os.path.join(os.path.dirname(pnevFileName), 'more_'+os.path.basename(pnevFileName))\n",
    "\n",
    "print(imfilename)\n",
    "print(pnevFileName)\n",
    "print(postName)\n",
    "print(moreName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a7922bdf-0e89-4b88-a366-7990f1749819"
    }
   },
   "source": [
    "## Load matlab variables: event-aligned traces, inhibitRois, outcomes,  choice, etc\n",
    "    - traces are set in set_aligned_traces.m matlab script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of stimulus-aligned traces: (55, 326, 178) (frames x units x trials)\n",
      "Current outcome: 119 correct choices; 37 incorrect choices\n"
     ]
    }
   ],
   "source": [
    "# Set traces_al_stim that is same as traces_al_stimAll except that in traces_al_stim some trials are set to nan, bc their stim duration is < \n",
    "# th_stim_dur or bc their go tone happens before ep(end) or bc their choice happened before ep(end). \n",
    "# But in traces_al_stimAll, all trials are included. \n",
    "# You need traces_al_stim for decoding the upcoming choice bc you average responses during ep and you want to \n",
    "# control for what happens there. But for trial-history analysis you average responses before stimOnset, so you \n",
    "# don't care about when go tone happened or how long the stimulus was. \n",
    "\n",
    "frameLength = 1000/30.9; # sec.\n",
    "    \n",
    "# Load time of some trial events    \n",
    "Data = scio.loadmat(postName, variable_names=['timeCommitCL_CR_Gotone', 'timeStimOnset', 'timeStimOffset', 'time1stSideTry'])\n",
    "timeCommitCL_CR_Gotone = np.array(Data.pop('timeCommitCL_CR_Gotone')).flatten().astype('float')\n",
    "timeStimOnset = np.array(Data.pop('timeStimOnset')).flatten().astype('float')\n",
    "timeStimOffset = np.array(Data.pop('timeStimOffset')).flatten().astype('float')\n",
    "time1stSideTry = np.array(Data.pop('time1stSideTry')).flatten().astype('float')\n",
    "\n",
    "\n",
    "# Load stim-aligned_allTrials traces, frames, frame of event of interest\n",
    "if trialHistAnalysis==0:\n",
    "    Data = scio.loadmat(postName, variable_names=['stimAl_noEarlyDec'],squeeze_me=True,struct_as_record=False)\n",
    "    eventI = Data['stimAl_noEarlyDec'].eventI - 1 # remember difference indexing in matlab and python!\n",
    "    traces_al_stimAll = Data['stimAl_noEarlyDec'].traces.astype('float')\n",
    "    time_aligned_stim = Data['stimAl_noEarlyDec'].time.astype('float')\n",
    "\n",
    "else:\n",
    "    Data = scio.loadmat(postName, variable_names=['stimAl_allTrs'],squeeze_me=True,struct_as_record=False)\n",
    "    eventI = Data['stimAl_allTrs'].eventI - 1 # remember difference indexing in matlab and python!\n",
    "    traces_al_stimAll = Data['stimAl_allTrs'].traces.astype('float')\n",
    "    time_aligned_stim = Data['stimAl_allTrs'].time.astype('float')\n",
    "    # time_aligned_stimAll = Data['stimAl_allTrs'].time.astype('float') # same as time_aligned_stim\n",
    "\n",
    "print 'size of stimulus-aligned traces:', np.shape(traces_al_stimAll), '(frames x units x trials)'\n",
    "DataS = Data\n",
    "\n",
    "\n",
    "\n",
    "# Load outcomes and choice (allResp_HR_LR) for the current trial\n",
    "# if trialHistAnalysis==0:\n",
    "Data = scio.loadmat(postName, variable_names=['outcomes', 'allResp_HR_LR'])\n",
    "outcomes = (Data.pop('outcomes').astype('float'))[0,:]\n",
    "# allResp_HR_LR = (Data.pop('allResp_HR_LR').astype('float'))[0,:]\n",
    "allResp_HR_LR = np.array(Data.pop('allResp_HR_LR')).flatten().astype('float')\n",
    "choiceVecAll = allResp_HR_LR+0;  # trials x 1;  1 for HR choice, 0 for LR choice. % choice of the current trial.    \n",
    "# choiceVecAll = np.transpose(allResp_HR_LR);  # trials x 1;  1 for HR choice, 0 for LR choice. % choice of the current trial.    \n",
    "print 'Current outcome: %d correct choices; %d incorrect choices' %(sum(outcomes==1), sum(outcomes==0))\n",
    "\n",
    "\n",
    "if trialHistAnalysis:\n",
    "    # Load trialHistory structure to get choice vector of the previous trial\n",
    "    Data = scio.loadmat(postName, variable_names=['trialHistory'],squeeze_me=True,struct_as_record=False)\n",
    "    choiceVec0All = Data['trialHistory'].choiceVec0.astype('float')\n",
    "\n",
    "\n",
    "    \n",
    "# Set trials strength and identify trials with stim strength of interest\n",
    "if trialHistAnalysis==0:\n",
    "    Data = scio.loadmat(postName, variable_names=['stimrate', 'cb'])\n",
    "    stimrate = np.array(Data.pop('stimrate')).flatten().astype('float')\n",
    "    cb = np.array(Data.pop('cb')).flatten().astype('float')\n",
    "\n",
    "    s = stimrate-cb; # how far is the stimulus rate from the category boundary?\n",
    "    if strength2ana == 'easy':\n",
    "        str2ana = (abs(s) >= (max(abs(s)) - thStimStrength));\n",
    "    elif strength2ana == 'hard':\n",
    "        str2ana = (abs(s) <= thStimStrength);\n",
    "    elif strength2ana == 'medium':\n",
    "        str2ana = ((abs(s) > thStimStrength) & (abs(s) < (max(abs(s)) - thStimStrength))); \n",
    "    else:\n",
    "        str2ana = np.full((1, np.shape(outcomes)[0]), True, dtype=bool).flatten();\n",
    "\n",
    "    print 'Number of trials with stim strength of interest = %i' %(str2ana.sum())\n",
    "    print 'Stim rates for training = {}'.format(np.unique(stimrate[str2ana]))\n",
    "\n",
    "    '''\n",
    "    # Set to nan those trials in outcomes and allRes that are nan in traces_al_stim\n",
    "    I = (np.argwhere((~np.isnan(traces_al_stim).sum(axis=0)).sum(axis=1)))[0][0] # first non-nan neuron\n",
    "    allTrs2rmv = np.argwhere(sum(np.isnan(traces_al_stim[:,I,:])))\n",
    "    print(np.shape(allTrs2rmv))\n",
    "\n",
    "    outcomes[allTrs2rmv] = np.nan\n",
    "    allResp_HR_LR[allTrs2rmv] = np.nan\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the time window for training SVM (ep) and traces_al_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch is [-485. -453. -421. -388. -356. -324. -291. -259. -227. -194. -162. -129.\n",
      "  -97.  -65.  -32.    0.] ms\n"
     ]
    }
   ],
   "source": [
    "traces_al_stim = traces_al_stimAll\n",
    "\n",
    "if trialHistAnalysis==1:    \n",
    "    # either of the two below (stimulus-aligned and initTone-aligned) would be fine\n",
    "    # eventI = DataI['initToneAl'].eventI\n",
    "    eventI = DataS['stimAl_allTrs'].eventI    \n",
    "    epEnd = eventI + epEnd_rel2stimon_fr #- 2 # to be safe for decoder training for trial-history analysis we go upto the frame before the stim onset\n",
    "    # epEnd = DataI['initToneAl'].eventI - 2 # to be safe for decoder training for trial-history analysis we go upto the frame before the initTone onset\n",
    "    ep = np.arange(epEnd+1)\n",
    "    print 'training epoch is {} ms'.format(np.round((ep-eventI)*frameLength))\n",
    "    \n",
    "    ep_ms = list(np.round((ep[[0,-1]]-eventI)*frameLength).astype(int)) # so it is the same format as ep_ms when trialHistAnalysis is 0\n",
    "    \n",
    "else:\n",
    "    # Set ep_ms if it is not provided: [choiceTime-300 choiceTime]ms # we also go 30ms back to make sure we are not right on the choice time!\n",
    "    # by doing this you wont need to set ii below.\n",
    "    \n",
    "    # We first set to nan timeStimOnset of trials that anyway wont matter bc their outcome is  not of interest. we do this to make sure these trials dont affect our estimate of ep_ms\n",
    "    if outcome2ana == 'corr':\n",
    "        timeStimOnset[outcomes!=1] = np.nan; # analyze only correct trials.\n",
    "    elif outcome2ana == 'incorr':\n",
    "        timeStimOnset[outcomes!=0] = np.nan; # analyze only incorrect trials.   \n",
    "        \n",
    "    if not 'ep_ms' in locals(): \n",
    "        ep_ms = [np.floor(np.nanmin(time1stSideTry-timeStimOnset))-30-300, np.floor(np.nanmin(time1stSideTry-timeStimOnset))-30]\n",
    "        print 'Training window: [%d %d] ms' %(ep_ms[0], ep_ms[1])\n",
    "    \n",
    "    epStartRel2Event = np.ceil(ep_ms[0]/frameLength); # the start point of the epoch relative to alignedEvent for training SVM. (500ms)\n",
    "    epEndRel2Event = np.ceil(ep_ms[1]/frameLength); # the end point of the epoch relative to alignedEvent for training SVM. (700ms)\n",
    "    ep = np.arange(eventI+epStartRel2Event, eventI+epEndRel2Event+1).astype(int); # frames on stimAl.traces that will be used for trainning SVM.   \n",
    "    print 'Training epoch relative to stimOnset is {} ms'.format(np.round((ep-eventI)*frameLength - frameLength/2)) # print center of frames in ms\n",
    "            \n",
    "        \n",
    "    #%% Exclude some trials from traces_al_stim\n",
    "    # This criteria makes sense if you want to be conservative; otherwise if ep=[1000 1300]ms, go tone will definitely be before ep end, and you cannot have the following criteria.\n",
    "    # Make sure in none of the trials Go-tone happened before the end of training window (ep)\n",
    "    i = (timeCommitCL_CR_Gotone - timeStimOnset) <= ep_ms[-1];\n",
    "    '''\n",
    "    if np.sum(i)>0:\n",
    "        print 'Excluding %i trials from timeStimOnset bc their goTone is earlier than ep end' %(np.sum(i))\n",
    "    #     timeStimOnset[i] = np.nan;  # by setting to nan, the aligned-traces of these trials will be computed as nan.\n",
    "    else:\n",
    "        print('No trials with go tone before the end of ep. Good :)')\n",
    "    '''\n",
    "        \n",
    "    # Make sure in none of the trials choice (1st side try) happened before the end of training window (ep)\n",
    "    ii = (time1stSideTry - timeStimOnset) <= ep_ms[-1];\n",
    "    if np.sum(ii)>0:\n",
    "        print 'Excluding %i trials from timeStimOnset bc their choice is earlier than ep end' %(np.sum(ii))\n",
    "    #     timeStimOnset[i] = np.nan;  # by setting to nan, the aligned-traces of these trials will be computed as nan.\n",
    "    else:\n",
    "        print('No trials with choice before the end of ep. Good :)')        \n",
    "\n",
    "\n",
    "    # Make sure trials that you use for SVM (decoding upcoming choice from\n",
    "    # neural responses during stimulus) have a certain stimulus duration. Of\n",
    "    # course stimulus at least needs to continue until the end of ep. \n",
    "    # go with either 900 or 800ms. Since the preference is to have at least\n",
    "    # ~100ms after ep which contains stimulus and without any go tones, go with 800ms\n",
    "    # bc in many sessions go tone happened early... so you will loose lots of\n",
    "    # trials if you go with 900ms.\n",
    "    # th_stim_dur = 800; # min stim duration to include a trial in timeStimOnset\n",
    "\n",
    "    if doPlots:\n",
    "        plt.figure\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(timeCommitCL_CR_Gotone - timeStimOnset, label = 'goTone')\n",
    "        plt.plot(timeStimOffset - timeStimOnset, 'r', label = 'stimOffset')\n",
    "        plt.plot(time1stSideTry - timeStimOnset, 'm', label = '1stSideTry')\n",
    "        plt.plot([1, np.shape(timeCommitCL_CR_Gotone)[0]],[th_stim_dur, th_stim_dur], 'g:', label = 'th_stim_dur')\n",
    "        plt.plot([1, np.shape(timeCommitCL_CR_Gotone)[0]],[ep_ms[-1], ep_ms[-1]], 'k:', label = 'epoch end')\n",
    "        plt.xlabel('Trial')\n",
    "        plt.ylabel('Time relative to stim onset (ms)')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, .7)) \n",
    "        # minStimDurNoGoTone = np.nanmin(timeCommitCL_CR_Gotone - timeStimOnset); # this is the duration after stim onset during which no go tone occurred for any of the trials.\n",
    "        # print 'minStimDurNoGoTone = %.2f ms' %minStimDurNoGoTone\n",
    "\n",
    "\n",
    "    # Exclude trials whose stim duration was < th_stim_dur\n",
    "    j = (timeStimOffset - timeStimOnset) < th_stim_dur;\n",
    "    if np.sum(j)>0:\n",
    "        print 'Excluding %i trials from timeStimOnset bc their stimDur < %dms' %(np.sum(j), th_stim_dur)\n",
    "    #     timeStimOnset[j] = np.nan;\n",
    "    else:\n",
    "        print 'No trials with stimDur < %dms. Good :)' %th_stim_dur\n",
    "\n",
    "\n",
    "\n",
    "    # Set trials to be removed from traces_al_stimAll    \n",
    "    # toRmv = (i+j+ii)!=0;  \n",
    "    toRmv = (j+ii)!=0; print 'Not excluding %i trials whose goTone is earlier than ep end' %sum(i)\n",
    "    print 'Final: %i trials excluded in traces_al_stim' %np.sum(toRmv)\n",
    "\n",
    "    \n",
    "    # Set traces_al_stim for SVM classification of current choice.     \n",
    "    traces_al_stim[:,:,toRmv] = np.nan\n",
    "#     traces_al_stim[:,:,outcomes==-1] = np.nan\n",
    "    # print(np.shape(traces_al_stim))\n",
    "\n",
    "\n",
    "    '''\n",
    "    # Set ep\n",
    "    if len(ep_ms)==0: # load ep from matlab\n",
    "        # Load stimulus-aligned traces, frames, frame of event of interest, and epoch over which we will average the responses to do SVM analysis\n",
    "        Data = scio.loadmat(postName, variable_names=['stimAl'],squeeze_me=True,struct_as_record=False)\n",
    "        # eventI = Data['stimAl'].eventI - 1 # remember difference indexing in matlab and python!\n",
    "        # traces_al_stim = Data['stimAl'].traces.astype('float') # traces_al_stim\n",
    "        # time_aligned_stim = Data['stimAl'].time.astype('float')\n",
    "\n",
    "        ep = Data['stimAl'].ep - 1\n",
    "        ep_ms = np.round((ep-eventI)*frameLength).astype(int)\n",
    "        \n",
    "    else: # set ep here:\n",
    "    '''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "dea0f2e6-c718-4e16-9104-a49426d7eecc"
    }
   },
   "outputs": [],
   "source": [
    "# Load 1stSideTry-aligned traces, frames, frame of event of interest\n",
    "# use firstSideTryAl_COM to look at changes-of-mind (mouse made a side lick without committing it)\n",
    "Data = scio.loadmat(postName, variable_names=['firstSideTryAl'],squeeze_me=True,struct_as_record=False)\n",
    "traces_al_1stSide = Data['firstSideTryAl'].traces.astype('float')\n",
    "time_aligned_1stSide = Data['firstSideTryAl'].time.astype('float')\n",
    "# print(np.shape(traces_al_1stSide))\n",
    "\n",
    "\n",
    "# Load goTone-aligned traces, frames, frame of event of interest\n",
    "# use goToneAl_noStimAft to make sure there was no stim after go tone.\n",
    "Data = scio.loadmat(postName, variable_names=['goToneAl'],squeeze_me=True,struct_as_record=False)\n",
    "traces_al_go = Data['goToneAl'].traces.astype('float')\n",
    "time_aligned_go = Data['goToneAl'].time.astype('float')\n",
    "# print(np.shape(traces_al_go))\n",
    "\n",
    "\n",
    "# Load reward-aligned traces, frames, frame of event of interest\n",
    "Data = scio.loadmat(postName, variable_names=['rewardAl'],squeeze_me=True,struct_as_record=False)\n",
    "traces_al_rew = Data['rewardAl'].traces.astype('float')\n",
    "time_aligned_rew = Data['rewardAl'].time.astype('float')\n",
    "# print(np.shape(traces_al_rew))\n",
    "\n",
    "\n",
    "# Load commitIncorrect-aligned traces, frames, frame of event of interest\n",
    "Data = scio.loadmat(postName, variable_names=['commitIncorrAl'],squeeze_me=True,struct_as_record=False)\n",
    "traces_al_incorrResp = Data['commitIncorrAl'].traces.astype('float')\n",
    "time_aligned_incorrResp = Data['commitIncorrAl'].time.astype('float')\n",
    "# print(np.shape(traces_al_incorrResp))\n",
    "\n",
    "\n",
    "# Load initiationTone-aligned traces, frames, frame of event of interest\n",
    "Data = scio.loadmat(postName, variable_names=['initToneAl'],squeeze_me=True,struct_as_record=False)\n",
    "traces_al_init = Data['initToneAl'].traces.astype('float')\n",
    "time_aligned_init = Data['initToneAl'].time.astype('float')\n",
    "# print(np.shape(traces_al_init))\n",
    "# DataI = Data\n",
    "'''\n",
    "if trialHistAnalysis:\n",
    "    # either of the two below (stimulus-aligned and initTone-aligned) would be fine\n",
    "    # eventI = DataI['initToneAl'].eventI\n",
    "    eventI = DataS['stimAl_allTrs'].eventI    \n",
    "    epEnd = eventI + epEnd_rel2stimon_fr #- 2 # to be safe for decoder training for trial-history analysis we go upto the frame before the stim onset\n",
    "    # epEnd = DataI['initToneAl'].eventI - 2 # to be safe for decoder training for trial-history analysis we go upto the frame before the initTone onset\n",
    "    ep = np.arange(epEnd+1)\n",
    "    print 'training epoch is {} ms'.format(np.round((ep-eventI)*frameLength))\n",
    "'''\n",
    "    \n",
    "\n",
    "# Load inhibitRois\n",
    "Data = scio.loadmat(moreName, variable_names=['inhibitRois'])\n",
    "inhibitRois = Data.pop('inhibitRois')[0,:]\n",
    "# print '%d inhibitory, %d excitatory; %d unsure class' %(np.sum(inhibitRois==1), np.sum(inhibitRois==0), np.sum(np.isnan(inhibitRois)))\n",
    "\n",
    "    \n",
    "# Set traces for specific neuron types: inhibitory, excitatory or all neurons\n",
    "if neuronType!=2:    \n",
    "    nt = (inhibitRois==neuronType) # 0: excitatory, 1: inhibitory, 2: all types.\n",
    "    # good_excit = inhibitRois==0;\n",
    "    # good_inhibit = inhibitRois==1;        \n",
    "    \n",
    "    traces_al_stim = traces_al_stim[:, nt, :];\n",
    "    traces_al_1stSide = traces_al_1stSide[:, nt, :];\n",
    "    traces_al_go = traces_al_go[:, nt, :];\n",
    "    traces_al_rew = traces_al_rew[:, nt, :];\n",
    "    traces_al_incorrResp = traces_al_incorrResp[:, nt, :];\n",
    "    traces_al_init = traces_al_init[:, nt, :];\n",
    "    traces_al_stimAll = traces_al_stimAll[:, nt, :];\n",
    "else:\n",
    "    nt = np.arange(np.shape(traces_al_1stSide)[1])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "26aea324-4089-47e6-8b09-902c6624dbaa"
    }
   },
   "source": [
    "## Set X (trials x neurons) and Y (trials x 1) for training the SVM classifier.\n",
    "    X matrix (size trials x neurons) that contains neural responses at different trials.\n",
    "    Y choice of high rate (modeled as 1) and low rate (modeled as 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2e12cfa5-8ba7-40c6-a9dd-81c4841ee78d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of spikeAveEp0 (trs x neurons):  (178, 326)\n"
     ]
    }
   ],
   "source": [
    "# Set choiceVec0  (Y: the response vector)\n",
    "\n",
    "if trialHistAnalysis:\n",
    "    choiceVec0 = choiceVec0All[:,iTiFlg] # choice on the previous trial for short (or long or all) ITIs\n",
    "    choiceVec0S = choiceVec0All[:,0]\n",
    "    choiceVec0L = choiceVec0All[:,1]\n",
    "else: # set choice for the current trial\n",
    "    choiceVec0 = allResp_HR_LR;  # trials x 1;  1 for HR choice, 0 for LR choice. % choice of the current trial.    \n",
    "    # choiceVec0 = np.transpose(allResp_HR_LR);  # trials x 1;  1 for HR choice, 0 for LR choice. % choice of the current trial.    \n",
    "    if outcome2ana == 'corr':\n",
    "        choiceVec0[outcomes!=1] = np.nan; # analyze only correct trials.\n",
    "    elif outcome2ana == 'incorr':\n",
    "        choiceVec0[outcomes!=0] = np.nan; # analyze only incorrect trials.   \n",
    "    \n",
    "    choiceVec0[~str2ana] = np.nan   \n",
    "    # Y = choiceVec0\n",
    "    # print(choiceVec0.shape)\n",
    "\n",
    "\n",
    "# Set spikeAveEp0  (X: the predictor matrix (trials x neurons) that shows average of spikes for a particular epoch for each trial and neuron.)\n",
    "\n",
    "if trialHistAnalysis:\n",
    "    # either of the two cases below should be fine (init-aligned traces or stim-aligned traces.)\n",
    "    spikeAveEp0 = np.transpose(np.nanmean(traces_al_stimAll[ep,:,:], axis=0)) # trials x neurons    \n",
    "    # spikeAveEp0 = np.transpose(np.nanmean(traces_al_init[ep,:,:], axis=0)) # trials x neurons    \n",
    "else:    \n",
    "    spikeAveEp0 = np.transpose(np.nanmean(traces_al_stim[ep,:,:], axis=0)) # trials x neurons    \n",
    "\n",
    "# X = spikeAveEp0;\n",
    "print 'Size of spikeAveEp0 (trs x neurons): ', spikeAveEp0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 high-rate trials, and 56 low-rate trials\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set trsExcluded and exclude them to set X and Y; trsExcluded are trials that are nan either in traces or in choice vector.\n",
    "\n",
    "'''\n",
    "#dirName = 'SVM_151102_001-002_ch2-PnevPanResults-160624-113108';\n",
    "dirName = 'SVM_151029_003_ch2-PnevPanResults-160426-191859';\n",
    "#dirName = '/home/farznaj/Shares/Churchland/data/fni17/imaging/151022/XY_fni17_151022 XY_lassoSVM.mat';\n",
    "Data = scio.loadmat(dirName, variable_names=['X', 'Y', 'time_aligned_stim', 'non_filtered', 'traces_al_1stSideTry', 'time_aligned_stim_1stSideTry']);\n",
    "X = Data.pop('X').astype('float')\n",
    "Y = np.squeeze(Data.pop('Y')).astype('int')\n",
    "time_aligned_stim = np.squeeze(Data.pop('time_aligned_stim')).astype('float')\n",
    "Xt = Data.pop('non_filtered').astype('float')\n",
    "Xt_choiceAl = Data.pop('traces_al_1stSideTry').astype('float')\n",
    "time_aligned_1stSide = np.squeeze(Data.pop('time_aligned_stim_1stSideTry')).astype('float')\n",
    "'''\n",
    "\n",
    "# Identify nan trials\n",
    "trsExcluded = (np.sum(np.isnan(spikeAveEp0), axis = 1) + np.isnan(choiceVec0)) != 0 # NaN trials # trsExcluded\n",
    "# print sum(trsExcluded), 'NaN trials'\n",
    "\n",
    "# Exclude nan trials\n",
    "X = spikeAveEp0[~trsExcluded,:]; # trials x neurons\n",
    "Y = choiceVec0[~trsExcluded];\n",
    "print '%d high-rate trials, and %d low-rate trials\\n' %(np.sum(Y==1), np.sum(Y==0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NsExcluded from file /home/farznaj/Shares/Churchland/data/fni17/imaging/151102/svm/svmPrevChoice_allN_allITIs_ep-485-0ms_r10_151102_001-002_ch2-PnevPanResults-161004-115936.mat\n",
      "18 = Final # non-active neurons\n",
      "Using 308 out of 326 neurons; Fraction excluded = 0.06\n",
      "\n",
      "35, 265, 26: #original inh, excit, unsure\n",
      "4, 13, 1: #excluded inh, excit, unsure\n",
      "0.11, 0.05, 0.04: fraction excluded inh, excit, unsure\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set NsExcluded : Identify neurons that did not fire in any of the trials (during ep) and then exclude them. Otherwise they cause problem for feature normalization.\n",
    "# thAct and thTrsWithSpike are parameters that you can play with.\n",
    "\n",
    "# If it is already saved, load it (the idea is to use the same NsExcluded for all the analyses of a session). Otherwise set it.\n",
    "if trialHistAnalysis==0:\n",
    "    svmnowname = 'svmCurrChoice_allN' + '_*-' + pnevFileName[-32:]\n",
    "else:\n",
    "    svmnowname = 'svmPrevChoice_allN_allITIs' + '_*-' + pnevFileName[-32:]\n",
    "svmName = glob.glob(os.path.join(os.path.dirname(pnevFileName), 'svm', svmnowname))\n",
    "svmName = sorted(svmName, key=os.path.getmtime)[::-1] # so the latest file is the 1st one.\n",
    "    \n",
    "\n",
    "if setNsExcluded==0 and np.shape(svmName)[0]!=0: # NsExcluded is already set and saved # 0: #    \n",
    "    svmName = svmName[0]  # get the latest file\n",
    "    print 'loading NsExcluded from file', svmName    \n",
    "    Data = scio.loadmat(svmName, variable_names=['NsExcluded'])\n",
    "    NsExcluded = Data.pop('NsExcluded')[0,:].astype('bool')\n",
    "    NsExcluded = NsExcluded[nt]      \n",
    "\n",
    "    stdX = np.std(X, axis = 0); # define stdX for all neurons; later we reset it only including active neurons\n",
    "    if min(stdX[~NsExcluded]) < thAct: # make sure the loaded NsExcluded makes sense; ie stdX of ~NsExcluded is above thAct\n",
    "        sys.exit(('min of stdX= %.8f; not supposed to be <%d (thAct)!'  %(min(stdX), thAct)))\n",
    "else:\n",
    "    \n",
    "    print 'NsExcluded not saved, so setting it here'\n",
    "    \n",
    "    if trialHistAnalysis and iTiFlg!=2:\n",
    "        # set X for short-ITI and long-ITI cases (XS,XL).\n",
    "        trsExcludedS = (np.sum(np.isnan(spikeAveEp0), axis = 1) + np.isnan(choiceVec0S)) != 0 \n",
    "        XS = spikeAveEp0[~trsExcludedS,:]; # trials x neurons\n",
    "        trsExcludedL = (np.sum(np.isnan(spikeAveEp0), axis = 1) + np.isnan(choiceVec0L)) != 0 \n",
    "        XL = spikeAveEp0[~trsExcludedL,:]; # trials x neurons\n",
    "\n",
    "        # Define NsExcluded as neurons with low stdX for either short ITI or long ITI trials. \n",
    "        # This is to make sure short and long ITI cases will include the same set of neurons.\n",
    "        stdXS = np.std(XS, axis = 0);\n",
    "        stdXL = np.std(XL, axis = 0);\n",
    "\n",
    "        NsExcluded = np.sum([stdXS < thAct, stdXL < thAct], axis=0)!=0 # if a neurons is non active for either short ITI or long ITI trials, exclude it.\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # Define NsExcluded as neurons with low stdX\n",
    "        stdX = np.std(X, axis = 0);\n",
    "        NsExcluded = stdX < thAct\n",
    "        # np.sum(stdX < thAct)\n",
    "\n",
    "        '''\n",
    "        # Set nonActiveNs, ie neurons whose average activity during ep is less than thAct.\n",
    "    #     spikeAveEpAveTrs = np.nanmean(spikeAveEp0, axis=0); # 1 x units % response of each neuron averaged across epoch ep and trials.\n",
    "        spikeAveEpAveTrs = np.nanmean(X, axis=0); # 1 x units % response of each neuron averaged across epoch ep and trials.\n",
    "        # thAct = 5e-4; # 1e-5 #quantile(spikeAveEpAveTrs, .1);\n",
    "        nonActiveNs = spikeAveEpAveTrs < thAct;\n",
    "        print '\\t%d neurons with ave activity in ep < %.5f' %(np.sum(nonActiveNs), thAct)\n",
    "        np.sum(nonActiveNs)\n",
    "\n",
    "        # Set NsFewTrActiv, ie neurons that are active in very few trials (by active I mean average activity during epoch ep)\n",
    "        # thTrsWithSpike = 1; # 3; # ceil(thMinFractTrs * size(spikeAveEp0,1)); % 30  % remove neurons with activity in <thSpTr trials.\n",
    "        nTrsWithSpike = np.sum(X > thAct, axis=0) # 0 # shows for each neuron, in how many trials the activity was above 0.\n",
    "        NsFewTrActiv = (nTrsWithSpike < thTrsWithSpike) # identify neurons that were active fewer than thTrsWithSpike.\n",
    "        print '\\t%d neurons are active in < %i trials' %(np.sum(NsFewTrActiv), thTrsWithSpike)\n",
    "\n",
    "        # Now set the final NxExcluded: (neurons to exclude)\n",
    "        NsExcluded = (NsFewTrActiv + nonActiveNs)!=0\n",
    "        '''\n",
    "\n",
    "print '%d = Final # non-active neurons' %(sum(NsExcluded))\n",
    "# a = size(spikeAveEp0,2) - sum(NsExcluded);\n",
    "print 'Using %d out of %d neurons; Fraction excluded = %.2f\\n' %(np.shape(spikeAveEp0)[1]-sum(NsExcluded), np.shape(spikeAveEp0)[1], sum(NsExcluded)/float(np.shape(spikeAveEp0)[1]))\n",
    "\n",
    "\n",
    "print '%i, %i, %i: #original inh, excit, unsure' %(np.sum(inhibitRois==1), np.sum(inhibitRois==0), np.sum(np.isnan(inhibitRois)))\n",
    "# Check what fraction of inhibitRois are excluded, compare with excitatory neurons.\n",
    "if neuronType==2:    \n",
    "    print '%i, %i, %i: #excluded inh, excit, unsure' %(np.sum(inhibitRois[NsExcluded]==1), np.sum(inhibitRois[NsExcluded]==0), np.sum(np.isnan(inhibitRois[NsExcluded])))\n",
    "    print '%.2f, %.2f, %.2f: fraction excluded inh, excit, unsure\\n' %(np.sum(inhibitRois[NsExcluded]==1)/float(np.sum(inhibitRois==1)), np.sum(inhibitRois[NsExcluded]==0)/float(np.sum(inhibitRois==0)), np.sum(np.isnan(inhibitRois[NsExcluded]))/float(np.sum(np.isnan(inhibitRois))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exclude non-active neurons from X and set inhRois (ie neurons that don't fire in any of the trials during ep)\n",
    "\n",
    "X = X[:,~NsExcluded]\n",
    "print np.shape(X)\n",
    "    \n",
    "# Set inhRois which is same as inhibitRois but with non-active neurons excluded. (it has same size as X)\n",
    "if neuronType==2:\n",
    "    inhRois = inhibitRois[~NsExcluded]\n",
    "    # print 'Number: inhibit = %d, excit = %d, unsure = %d' %(np.sum(inhRois==1), np.sum(inhRois==0), np.sum(np.isnan(inhRois)))\n",
    "    # print 'Fraction: inhibit = %.2f, excit = %.2f, unsure = %.2f' %(fractInh, fractExc, fractUn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  If number of neurons is more than 95% of trial numbers, identify n random neurons, where n= 0.95 * number of trials. This is to make sure we have more observations (trials) than features (neurons)\n",
    "\n",
    "nTrs = np.shape(X)[0]\n",
    "nNeuronsOrig = np.shape(X)[1]\n",
    "nNeuronsNow = np.int(np.floor(nTrs * .95))\n",
    "\n",
    "if nNeuronsNow < nNeuronsOrig:\n",
    "    if neuronType==2:\n",
    "        fractInh = np.sum(inhRois==1) / float(nNeuronsOrig)\n",
    "        fractExc = np.sum(inhRois==0) / float(nNeuronsOrig)\n",
    "        fractUn = np.sum(np.isnan(inhRois)) / float(nNeuronsOrig)\n",
    "        print 'Number: inhibit = %d, excit = %d, unsure = %d' %(np.sum(inhRois==1), np.sum(inhRois==0), np.sum(np.isnan(inhRois)))\n",
    "        print 'Fraction: inhibit = %.2f, excit = %.2f, unsure = %.2f' %(fractInh, fractExc, fractUn)\n",
    "    elif neuronType==0: # exc\n",
    "        fractInh = 0;\n",
    "        fractExc = 1;\n",
    "        fractUn = 0;\n",
    "    elif neuronType==1: # inh\n",
    "        fractInh = 1;\n",
    "        fractExc = 0;\n",
    "        fractUn = 0;    \n",
    "\n",
    "    # Define how many neurons you need to pick from each pool of inh, exc, unsure.\n",
    "    nInh = int(np.ceil(fractInh*nNeuronsNow))\n",
    "    nExc = int(np.ceil(fractExc*nNeuronsNow))\n",
    "    nUn = nNeuronsNow - (nInh + nExc) # fractUn*nNeuronsNow\n",
    "\n",
    "    print '\\nThere are', nTrs, 'trials; So selecting', nNeuronsNow, 'neurons out of', nNeuronsOrig\n",
    "    print '%i, %i, %i: number of selected inh, excit, unsure' %(nInh, nExc, nUn)\n",
    "\n",
    "    # Select nInh random indeces out of the inhibibory pool\n",
    "    inhI = np.argwhere(inhRois==1)\n",
    "    inhNow = rng.permutation(inhI)[0:nInh].flatten() # random indeces\n",
    "\n",
    "    # Select nExc random indeces out of the excitatory pool\n",
    "    excI = np.argwhere(inhRois==0)\n",
    "    excNow = rng.permutation(excI)[0:nExc].flatten()\n",
    "\n",
    "    # Select nUn random indeces out of the unsure pool\n",
    "    unI = np.argwhere(np.isnan(inhRois))\n",
    "    unNow = rng.permutation(unI)[0:nUn].flatten()\n",
    "\n",
    "    # Put all the 3 groups together \n",
    "    neuronsNow = np.sort(np.concatenate([inhNow,excNow,unNow]), axis=None)\n",
    "    np.shape(neuronsNow)\n",
    "    # neuronsNow\n",
    "    # np.max(neuronsNow)\n",
    "\n",
    "    # Define a logical array with 1s for randomly selected neurons (length = number of neurons in X (after excluding NsExcluded))\n",
    "    NsRand = np.arange(np.shape(X)[1])\n",
    "    NsRand = np.in1d(NsRand, neuronsNow)\n",
    "    # np.shape(NsRand)\n",
    "    # NsRand    \n",
    "else: # if number of neurons is already <= .95*numTrials, include all neurons.\n",
    "    print 'Not doing random selection of neurons (nNeurons=%d already fewer than .95*nTrials=%d)' %(np.shape(X)[1], nTrs)\n",
    "    NsRand = np.ones(np.shape(X)[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set X and inhRois only for the randomly selected set of neurons\n",
    "\n",
    "X = X[:,NsRand]\n",
    "if neuronType==2:\n",
    "    inhRois = inhRois[NsRand]\n",
    "\n",
    "if windowAvgFlg==0:\n",
    "    a = np.transpose(traces_al_stim[ep,:,:][:,~NsExcluded,:][:,:,~trsExcluded], (0,2,1))  # ep_frames x trials x units\n",
    "    a = a[:,:,neuronsNow]\n",
    "    X = np.reshape(a, (ep.shape[0]*(~trsExcluded).sum(), (~NsExcluded).sum())) # (ep_frames x trials) x units\n",
    "\n",
    "    Y = np.tile(np.reshape(choiceVec0[~trsExcluded], (1,-1)), (ep.shape[0], 1)).flatten()    \n",
    "\n",
    "    \n",
    "# Handle imbalance in the number of trials:\n",
    "# unlike matlab, it doesn't seem to be a problem here... so we don't make trial numbers of HR and LR the same.\n",
    "    \n",
    "    \n",
    "# Print some numbers\n",
    "numDataPoints = X.shape[0] \n",
    "print '# data points = %d' %numDataPoints\n",
    "# numTrials = (~trsExcluded).sum()\n",
    "# numNeurons = (~NsExcluded).sum()\n",
    "numTrials, numNeurons = X.shape\n",
    "print '%d trials; %d neurons' %(numTrials, numNeurons)\n",
    "# print ' The data has %d frames recorded from %d neurons at %d trials' %Xt.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Center and normalize X: feature normalization and scaling: to remove effects related to scaling and bias of each neuron, we need to zscore data (i.e., make data mean 0 and variance 1 for each neuron) \n",
    "\n",
    "meanX = np.mean(X, axis = 0);\n",
    "stdX = np.std(X, axis = 0);\n",
    "# normalize X\n",
    "X = (X-meanX)/stdX;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "88f9eee6-cc78-4dcf-9fe2-3d6e803617ff"
    }
   },
   "outputs": [],
   "source": [
    "if doPlots:\n",
    "    plt.figure\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(meanX)\n",
    "    plt.ylabel('meanX \\n(ep-ave FR, mean of trials)')\n",
    "    plt.title('min = %.6f' %(np.min(meanX)))\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(stdX)\n",
    "    plt.ylabel('stdX \\n(ep-ave FR, std of trials)')\n",
    "    plt.xlabel('neurons')\n",
    "    plt.title('min = %.6f' %(np.min(stdX)))\n",
    "\n",
    "    plt.tight_layout() #(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "    # plt.subplots_adjust(hspace=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the traces that will be used for projections and plotting \n",
    "    Traces are of size (frames x neurons x trials)\n",
    "    Choose trials that will be used for projections (trs4project = 'trained', 'all', 'corr', 'incorr')\n",
    "    Remove non-active neurons\n",
    "    Do feature normalization and scaling for the traces (using mean and sd of X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trs4project = 'incorr' # 'trained', 'all', 'corr', 'incorr'\n",
    "\n",
    "# Data = scio.loadmat(postName, variable_names=['outcomes', 'allResp_HR_LR'])\n",
    "# choiceVecAll = (Data.pop('allResp_HR_LR').astype('float'))[0,:]\n",
    "\n",
    "# Set trials that will be used for projection traces\n",
    "\n",
    "if trs4project=='all': \n",
    "    Xt = traces_al_stim\n",
    "    Xt_choiceAl = traces_al_1stSide\n",
    "    Xt_goAl = traces_al_go\n",
    "    Xt_rewAl = traces_al_rew\n",
    "    Xt_incorrRespAl = traces_al_incorrResp\n",
    "    Xt_initAl = traces_al_init\n",
    "    Xt_stimAl_all = traces_al_stimAll\n",
    "    choiceVecNow = choiceVecAll\n",
    "elif trs4project=='trained':\n",
    "    Xt = traces_al_stim[:, :, ~trsExcluded];\n",
    "    Xt_choiceAl = traces_al_1stSide[:, :, ~trsExcluded];\n",
    "    Xt_goAl = traces_al_go[:, :, ~trsExcluded];\n",
    "    Xt_rewAl = traces_al_rew[:, :, ~trsExcluded];\n",
    "    Xt_incorrRespAl = traces_al_incorrResp[:, :, ~trsExcluded];\n",
    "    Xt_initAl = traces_al_init[:, :, ~trsExcluded];\n",
    "    Xt_stimAl_all = traces_al_stimAll[:, :, ~trsExcluded];\n",
    "    choiceVecNow = Y    \n",
    "elif trs4project=='corr':\n",
    "    Xt = traces_al_stim[:, :, outcomes==1];\n",
    "    Xt_choiceAl = traces_al_1stSide[:, :, outcomes==1];\n",
    "    Xt_goAl = traces_al_go[:, :, outcomes==1];\n",
    "    Xt_rewAl = traces_al_rew[:, :, outcomes==1];\n",
    "    Xt_incorrRespAl = traces_al_incorrResp[:, :, outcomes==1];\n",
    "    Xt_initAl = traces_al_init[:, :, outcomes==1];\n",
    "    Xt_stimAl_all = traces_al_stimAll[:, :, outcomes==1];\n",
    "    choiceVecNow = choiceVecAll[outcomes==1]\n",
    "    \n",
    "elif trs4project=='incorr':\n",
    "    Xt = traces_al_stim[:, :, outcomes==0];\n",
    "    Xt_choiceAl = traces_al_1stSide[:, :, outcomes==0];\n",
    "    Xt_goAl = traces_al_go[:, :, outcomes==0];\n",
    "    Xt_rewAl = traces_al_rew[:, :, outcomes==0];\n",
    "    Xt_incorrRespAl = traces_al_incorrResp[:, :, outcomes==0];\n",
    "    Xt_initAl = traces_al_init[:, :, outcomes==0];\n",
    "    Xt_stimAl_all = traces_al_stimAll[:, :, outcomes==0];\n",
    "    choiceVecNow = choiceVecAll[outcomes==0]\n",
    "    \n",
    "## Xt = traces_al_stim[:, :, np.sum(np.sum(np.isnan(traces_al_stim), axis =0), axis =0)==0];\n",
    "## Xt_choiceAl = traces_al_1stSide[:, :, np.sum(np.sum(np.isnan(traces_al_1stSide), axis =0), axis =0)==0];\n",
    "\n",
    "\n",
    "\n",
    "# Exclude non-active neurons (ie neurons that don't fire in any of the trials during ep)\n",
    "Xt = Xt[:,~NsExcluded,:]\n",
    "Xt_choiceAl = Xt_choiceAl[:,~NsExcluded,:]\n",
    "Xt_goAl = Xt_goAl[:,~NsExcluded,:]\n",
    "Xt_rewAl = Xt_rewAl[:,~NsExcluded,:]\n",
    "Xt_incorrRespAl = Xt_incorrRespAl[:,~NsExcluded,:]\n",
    "Xt_initAl = Xt_initAl[:,~NsExcluded,:]\n",
    "Xt_stimAl_all = Xt_stimAl_all[:,~NsExcluded,:]\n",
    "    \n",
    "# Only include the randomly selected set of neurons\n",
    "Xt = Xt[:,NsRand,:]\n",
    "Xt_choiceAl = Xt_choiceAl[:,NsRand,:]\n",
    "Xt_goAl = Xt_goAl[:,NsRand,:]\n",
    "Xt_rewAl = Xt_rewAl[:,NsRand,:]\n",
    "Xt_incorrRespAl = Xt_incorrRespAl[:,NsRand,:]\n",
    "Xt_initAl = Xt_initAl[:,NsRand,:]\n",
    "Xt_stimAl_all = Xt_stimAl_all[:,NsRand,:]\n",
    "\n",
    "\n",
    "\n",
    "# Divide data into high-rate (modeled as 1) and low-rate (modeled as 0) trials\n",
    "hr_trs = (choiceVecNow==1)\n",
    "lr_trs = (choiceVecNow==0)\n",
    "# print 'Projection traces have %d high-rate trials, and %d low-rate trials' %(np.sum(hr_trs), np.sum(lr_trs))\n",
    "    \n",
    "    \n",
    "\n",
    "# window of training (ep)\n",
    "win = (ep-eventI)*frameLength\n",
    "\n",
    "# Plot stim-aligned averages after centering and normalization\n",
    "if doPlots:\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    a1 = np.nanmean(Xt[:, :, hr_trs],  axis=1) # frames x trials (average across neurons)\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xt[:, :, lr_trs],  axis=1) # frames x trials (average across neurons)\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    mn = np.concatenate([tr1,tr0]).min()\n",
    "    mx = np.concatenate([tr1,tr0]).max()\n",
    "    plt.plot([win[0], win[0]], [mn, mx], 'g-.') # mark the begining and end of training window\n",
    "    plt.plot([win[-1], win[-1]], [mn, mx], 'g-.')\n",
    "    plt.fill_between(time_aligned_stim, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_stim, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_stim, tr0, 'r', label = 'low rate')\n",
    "    # plt.plot(time_aligned_stim, np.nanmean(Xt[:, :, lr_trs],  axis = (1, 2)), 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_stim, np.nanmean(Xt[:, :, hr_trs],  axis = (1, 2)), 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to stimulus onset (ms)')\n",
    "    plt.title('Population average - raw')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## Feature normalization and scaling\n",
    "\n",
    "# normalize stim-aligned traces\n",
    "T, N, C = Xt.shape\n",
    "Xt_N = np.reshape(Xt.transpose(0 ,2 ,1), (T*C, N), order = 'F')\n",
    "Xt_N = (Xt_N-meanX)/stdX\n",
    "Xt = np.reshape(Xt_N, (T, C, N), order = 'F').transpose(0 ,2 ,1)\n",
    "\n",
    "# normalize goTome-aligned traces\n",
    "Tg, Ng, Cg = Xt_goAl.shape\n",
    "Xtg_N = np.reshape(Xt_goAl.transpose(0 ,2 ,1), (Tg*Cg, Ng), order = 'F')\n",
    "Xtg_N = (Xtg_N-meanX)/stdX\n",
    "Xtg = np.reshape(Xtg_N, (Tg, Cg, Ng), order = 'F').transpose(0 ,2 ,1)\n",
    "\n",
    "# normalize choice-aligned traces\n",
    "Tc, Nc, Cc = Xt_choiceAl.shape\n",
    "Xtc_N = np.reshape(Xt_choiceAl.transpose(0 ,2 ,1), (Tc*Cc, Nc), order = 'F')\n",
    "Xtc_N = (Xtc_N-meanX)/stdX\n",
    "Xtc = np.reshape(Xtc_N, (Tc, Cc, Nc), order = 'F').transpose(0 ,2 ,1)\n",
    "\n",
    "# normalize reward-aligned traces\n",
    "Tr, Nr, Cr = Xt_rewAl.shape\n",
    "Xtr_N = np.reshape(Xt_rewAl.transpose(0 ,2 ,1), (Tr*Cr, Nr), order = 'F')\n",
    "Xtr_N = (Xtr_N-meanX)/stdX\n",
    "Xtr = np.reshape(Xtr_N, (Tr, Cr, Nr), order = 'F').transpose(0 ,2 ,1)\n",
    "\n",
    "# normalize commitIncorrect-aligned traces\n",
    "Tp, Np, Cp = Xt_incorrRespAl.shape\n",
    "Xtp_N = np.reshape(Xt_incorrRespAl.transpose(0 ,2 ,1), (Tp*Cp, Np), order = 'F')\n",
    "Xtp_N = (Xtp_N-meanX)/stdX\n",
    "Xtp = np.reshape(Xtp_N, (Tp, Cp, Np), order = 'F').transpose(0 ,2 ,1)\n",
    "\n",
    "# normalize stimAll-aligned traces\n",
    "Tsa, Nsa, Csa = Xt_stimAl_all.shape\n",
    "Xtsa_N = np.reshape(Xt_stimAl_all.transpose(0 ,2 ,1), (Tsa*Csa, Nsa), order = 'F')\n",
    "Xtsa_N = (Xtsa_N-meanX)/stdX\n",
    "Xtsa = np.reshape(Xtsa_N, (Tsa, Csa, Nsa), order = 'F').transpose(0 ,2 ,1)\n",
    "\n",
    "# normalize init-aligned traces\n",
    "Ti, Ni, Ci = Xt_initAl.shape\n",
    "Xti_N = np.reshape(Xt_initAl.transpose(0 ,2 ,1), (Ti*Ci, Ni), order = 'F')\n",
    "Xti_N = (Xti_N-meanX)/stdX\n",
    "Xti = np.reshape(Xti_N, (Ti, Ci, Ni), order = 'F').transpose(0 ,2 ,1)    \n",
    "\n",
    "np.shape(Xt)\n",
    "\n",
    "\n",
    "# window of training (ep)\n",
    "# win = (ep-eventI)*frameLength\n",
    "\n",
    "# Plot stim-aligned averages after centering and normalization\n",
    "if doPlots:\n",
    "#     plt.figure()\n",
    "    plt.subplot(1,2,2)\n",
    "    a1 = np.nanmean(Xt[:, :, hr_trs],  axis=1) # frames x trials (average across neurons)\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xt[:, :, lr_trs],  axis=1) # frames x trials (average across neurons)\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    mn = np.concatenate([tr1,tr0]).min()\n",
    "    mx = np.concatenate([tr1,tr0]).max()\n",
    "    plt.plot([win[0], win[0]], [mn, mx], 'g-.') # mark the begining and end of training window\n",
    "    plt.plot([win[-1], win[-1]], [mn, mx], 'g-.')\n",
    "    plt.fill_between(time_aligned_stim, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_stim, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_stim, tr0, 'r', label = 'low rate')\n",
    "    # plt.plot(time_aligned_stim, np.nanmean(Xt[:, :, lr_trs],  axis = (1, 2)), 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_stim, np.nanmean(Xt[:, :, hr_trs],  axis = (1, 2)), 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to stimulus onset (ms)')\n",
    "    plt.title('Population average - normalized')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a553ab9-ddea-4aef-a13c-a882046908e4"
    }
   },
   "source": [
    "## Identify the best regularization parameter\n",
    "    Perform 10-fold cross validation to obtain the best regularization parameter\n",
    "        More specifically: \"crossValidateModel\" divides data into training and test datasets. It calls linearSVM.py, which does linear SVM using XTrain, and returns percent class loss for XTrain and XTest.\n",
    "    This procedure gets repeated for numSamples (100 times) for each value of regulariazation parameter. \n",
    "    An average across all 100 samples is computed to find the minimum test class loss.\n",
    "    Best regularization parameter is defined as the smallest regularization parameter whose test-dataset class loss is within mean+sem of minimum test class loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0f95662b-968a-4d3c-a81c-d9243ac0b070"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numSamples = 10; # number of iterations for finding the best c (inverse of regularization parameter)\n",
    "\n",
    "regType = 'l1'\n",
    "kfold = 10;\n",
    "if regType == 'l1':\n",
    "    print '\\nRunning l1 svm classification\\r' \n",
    "    # cvect = 10**(np.arange(-4, 6,0.2))/numTrials;\n",
    "    cvect = 10**(np.arange(-4, 6,0.2))/numDataPoints;\n",
    "elif regType == 'l2':\n",
    "    print '\\nRunning l2 svm classification\\r' \n",
    "    cvect = 10**(np.arange(-4, 6,0.2));\n",
    "\n",
    "print 'try the following regularization values: \\n', cvect\n",
    "# formattedList = ['%.2f' % member for member in cvect]\n",
    "# print 'try the following regularization values = \\n', formattedList\n",
    "\n",
    "perClassErrorTrain = np.ones((numSamples, len(cvect)))+np.nan;\n",
    "perClassErrorTest = np.ones((numSamples, len(cvect)))+np.nan;\n",
    "for s in range(numSamples):\n",
    "    for i in range(len(cvect)):\n",
    "        if regType == 'l1':\n",
    "            summary =  crossValidateModel(X, Y, linearSVM, kfold = kfold, l1 = cvect[i])\n",
    "        elif regType == 'l2':\n",
    "            summary =  crossValidateModel(X, Y, linearSVM, kfold = kfold, l2 = cvect[i])\n",
    "\n",
    "        perClassErrorTrain[s, i] = summary.perClassErrorTrain;\n",
    "        perClassErrorTest[s, i] = summary.perClassErrorTest;\n",
    "\n",
    "meanPerClassErrorTrain = np.mean(perClassErrorTrain, axis = 0);\n",
    "semPerClassErrorTrain = np.std(perClassErrorTrain, axis = 0)/np.sqrt(numSamples);\n",
    "\n",
    "meanPerClassErrorTest = np.mean(perClassErrorTest, axis = 0);\n",
    "semPerClassErrorTest = np.std(perClassErrorTest, axis = 0)/np.sqrt(numSamples);\n",
    "ix = np.argmin(meanPerClassErrorTest);\n",
    "cbest = cvect[meanPerClassErrorTest <= (meanPerClassErrorTest[ix]+semPerClassErrorTest[ix])];\n",
    "cbest = cbest[0]; # best regularization term based on minError+SE criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0939f724-98b6-4f51-b951-21f406be6202"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##%%%%%% plot coss-validation results\n",
    "if doPlots:\n",
    "    print 'Best c (inverse of regularization parameter) = %.2f' %cbest\n",
    "    plt.figure('cross validation')\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.fill_between(cvect, meanPerClassErrorTrain-semPerClassErrorTrain, meanPerClassErrorTrain+ semPerClassErrorTrain, alpha=0.5, edgecolor='k', facecolor='k')\n",
    "    plt.fill_between(cvect, meanPerClassErrorTest-semPerClassErrorTest, meanPerClassErrorTest+ semPerClassErrorTest, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(cvect, meanPerClassErrorTrain, 'k', label = 'training')\n",
    "    plt.plot(cvect, meanPerClassErrorTest, 'r', label = 'validation')\n",
    "    plt.plot(cvect[cvect==cbest], meanPerClassErrorTest[cvect==cbest], 'bo')\n",
    "    plt.xlim([cvect[1], cvect[-1]])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('c (inverse of regularization parameter)')\n",
    "    plt.ylabel('classification error (%)')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, .7))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2e20583c-e989-413c-ad4c-9d26659fd76a"
    }
   },
   "source": [
    "## Train SVM model using the best regularization parameter\n",
    "    All data in X are used for training.\n",
    "    linear_svm is the trained SVM model that includes weights (w) and intercept (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "03f2365a-c960-4015-86f5-94108bb4c62b"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if regType == 'l1':\n",
    "    linear_svm = svm.LinearSVC(C = cbest, loss='squared_hinge', penalty='l1', dual=False)\n",
    "elif regType == 'l2':\n",
    "    linear_svm = svm.LinearSVC(C = cbest, loss='squared_hinge', penalty='l2', dual=True)\n",
    "\n",
    "linear_svm.fit(X, Y)\n",
    "    \n",
    "w = np.squeeze(linear_svm.coef_);\n",
    "b = linear_svm.intercept_;\n",
    "\n",
    "trainE = abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100;\n",
    "\n",
    "# keep a copy of linear_svm\n",
    "import copy\n",
    "linear_svm_0 = copy.deepcopy(linear_svm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot weights\n",
    "if doPlots:\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(w[np.argsort(abs(w))[::-1]], 'k.', label = 'weights')\n",
    "    plt.plot(np.ones(len(w))*b, 'k--', label = 'bias')\n",
    "    plt.xlabel('sorted neurons')\n",
    "    plt.legend()\n",
    "    plt.title(('Training error: %.2f %%' %(trainE)))\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(w, 20,orientation='horizontal', color = 'k')\n",
    "    plt.ylabel('weights')\n",
    "    plt.xlabel('count')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # print abs(((np.dot(X,w)+b)>0).astype('float')-Y.astype('float')).sum()/len(Y)*100 # this is the prediction formula\n",
    "    print 'Fraction of non-zero weight neurons = %.2f' %(np.mean(w!=0))\n",
    "    print 'Training error = %.2f%%' %trainE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0fdae7f1-1443-4fbd-b22e-74db1a4920b8"
    }
   },
   "source": [
    "## Null distribution using shuffles\n",
    "    Compute distritbutions of class loss for train and test datasets by fitting SVM for 100 times.\n",
    "    Do this for both actual data and shuffled data (ie data in which Y is shuffled but X is not to serve as null distribution.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "824a3cbe-9626-4d39-b8c9-f43a6cce3bbb"
    }
   },
   "outputs": [],
   "source": [
    "numShuffles = 100\n",
    "summary_data = [];\n",
    "summary_shfl = [];\n",
    "perClassErrorTrain_data = [];\n",
    "perClassErrorTest_data = []\n",
    "perClassErrorTrain_shfl = [];\n",
    "perClassErrorTest_shfl = [];\n",
    "w_data = []\n",
    "b_data = []\n",
    "w_shfl = []\n",
    "b_shfl = []\n",
    "permIxsList = [];\n",
    "for i in range(numShuffles):\n",
    "    # permIxs = rng.permutation(numTrials);\n",
    "    permIxs = rng.permutation(numDataPoints);\n",
    "    permIxsList.append(permIxs);\n",
    "    if regType == 'l1':\n",
    "        summary_data.append(crossValidateModel(X, Y, linearSVM, kfold = kfold, l1 = cbest))\n",
    "        summary_shfl.append(crossValidateModel(X, Y[permIxs], linearSVM, kfold = kfold, l1 = cbest))\n",
    "    elif regType == 'l2':\n",
    "        summary_data.append(crossValidateModel(X, Y, linearSVM, kfold = kfold, l2 = cbest))\n",
    "        summary_shfl.append(crossValidateModel(X, Y[permIxs], linearSVM, kfold = kfold, l2 = cbest))\n",
    "        \n",
    "    perClassErrorTrain_data.append(summary_data[i].perClassErrorTrain);\n",
    "    perClassErrorTest_data.append(summary_data[i].perClassErrorTest);\n",
    "    w_data.append(np.squeeze(summary_data[i].model.coef_));\n",
    "    b_data.append(summary_data[i].model.intercept_);\n",
    "        \n",
    "    perClassErrorTrain_shfl.append(summary_shfl[i].perClassErrorTrain);\n",
    "    perClassErrorTest_shfl.append(summary_shfl[i].perClassErrorTest);\n",
    "    w_shfl.append(np.squeeze(summary_shfl[i].model.coef_));\n",
    "    b_shfl.append(summary_shfl[i].model.intercept_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d0236c49-3b10-4062-9996-42cacbc4ef0d"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pvalueTrain = ttest2(perClassErrorTrain_data, perClassErrorTrain_shfl, tail = 'left');\n",
    "pvalueTest = ttest2(perClassErrorTest_data, perClassErrorTest_shfl, tail = 'left');\n",
    "\n",
    "print 'Training error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(perClassErrorTrain_data), np.mean(perClassErrorTrain_shfl), pvalueTrain)\n",
    "print 'Testing error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(perClassErrorTest_data), np.mean(perClassErrorTest_shfl), pvalueTest)\n",
    "\n",
    "\n",
    "# Plot the histograms\n",
    "if doPlots:\n",
    "    binEvery = 3; # bin width\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(perClassErrorTrain_data, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "    plt.hist(perClassErrorTrain_shfl, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "    plt.xlabel('Training classification error (%)')\n",
    "    plt.ylabel('count')\n",
    "    plt.title('Mean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(perClassErrorTrain_data), np.mean(perClassErrorTrain_shfl), pvalueTrain), fontsize = 10)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(perClassErrorTest_data, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "    plt.hist(perClassErrorTest_shfl, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "    plt.legend()\n",
    "    plt.xlabel('Testing classification error (%)')\n",
    "    plt.title('Mean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(perClassErrorTest_data), np.mean(perClassErrorTest_shfl), pvalueTest), fontsize = 10)\n",
    "    plt.ylabel('count')\n",
    "    plt.tight_layout(pad=0.4, w_pad=1.5, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f3e82548-ca9a-4da9-8a9a-6d748191fddd"
    }
   },
   "source": [
    "## Project traces onto SVM weights\n",
    "    Stimulus-aligned, choice-aligned, etc traces projected onto SVM fitted weights.\n",
    "    More specifically, project traces of all trials onto normalized w (ie SVM weights computed from fitting model using X and Y of all trials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doPlots:\n",
    "    # w = np.zeros(numNeurons)\n",
    "    w_normalized = w/sci.linalg.norm(w);\n",
    "\n",
    "    # stim-aligned traces\n",
    "    # XtN_w = np.dot(Xt_N, w_normalized);\n",
    "    # Xt_w = np.reshape(XtN_w, (T,C), order='F');\n",
    "    # I think below can replace above, test it....\n",
    "    XtN_w = np.dot(Xtsa_N, w_normalized);\n",
    "    Xt_w = np.reshape(XtN_w, (Tsa,Csa), order='F');\n",
    "\n",
    "\n",
    "    # goTone-aligned \n",
    "    XtgN_w = np.dot(Xtg_N, w_normalized);\n",
    "    Xtg_w = np.reshape(XtgN_w, (Tg,Cg), order='F');\n",
    "\n",
    "    # choice-aligned \n",
    "    XtcN_w = np.dot(Xtc_N, w_normalized);\n",
    "    Xtc_w = np.reshape(XtcN_w, (Tc,Cc), order='F');\n",
    "\n",
    "    # reward-aligned \n",
    "    XtrN_w = np.dot(Xtr_N, w_normalized);\n",
    "    Xtr_w = np.reshape(XtrN_w, (Tr,Cr), order='F');\n",
    "\n",
    "    # incommitResp-aligned \n",
    "    XtpN_w = np.dot(Xtp_N, w_normalized);\n",
    "    Xtp_w = np.reshape(XtpN_w, (Tp,Cp), order='F');\n",
    "\n",
    "    # initTone-aligned \n",
    "    XtiN_w = np.dot(Xti_N, w_normalized);\n",
    "    Xti_w = np.reshape(XtiN_w, (Ti,Ci), order='F');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot projections and raw averages of neural population responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7d6ee65e-2628-49b7-a725-a506424b141d"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if doPlots:\n",
    "    # window of training (ep)\n",
    "    # win = (ep-eventI)*frameLength\n",
    "\n",
    "    # init-aligned projections and raw average\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xti_w[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xti_w[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xti_w[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xti_w[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_init, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_init, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_init, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_init, tr0, 'r', label = 'low rate')\n",
    "    # tr1=np.nanmean(Xtc_w[:, hr_trs],  axis = 1)\n",
    "    # tr0=np.nanmean(Xtc_w[:, lr_trs],  axis = 1)\n",
    "    # plt.plot(time_aligned_1stSide, tr1, 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_1stSide, tr0, 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to init tone (ms)')\n",
    "    plt.title('Projections onto classifier weights')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    a1 = np.nanmean(Xti[:, :, hr_trs],  axis=1) # frames x trials\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xti[:, :, lr_trs],  axis=1) # frames x trials\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_init, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_init, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_init, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_init, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to init tone (ms)')\n",
    "    plt.title('Population average')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # stim-aligned projections and raw average\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xt_w[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xt_w[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xt_w[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xt_w[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_stim, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_stim, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_stim, tr0, 'r', label = 'low rate')\n",
    "    mn = np.concatenate([tr1,tr0]).min()\n",
    "    mx = np.concatenate([tr1,tr0]).max()\n",
    "    plt.plot([win[0], win[0]], [mn, mx], 'g-.') # mark the begining and end of training window\n",
    "    plt.plot([win[-1], win[-1]], [mn, mx], 'g-.')\n",
    "    plt.xlabel('time aligned to stimulus onset (ms)')\n",
    "    plt.title('Projections onto classifier weights')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2) # I think you should use Xtsa here to make it compatible with the plot above.\n",
    "    a1 = np.nanmean(Xt[:, :, hr_trs],  axis=1) # frames x trials (average across neurons)\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xt[:, :, lr_trs],  axis=1) # frames x trials (average across neurons)\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_stim, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_stim, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_stim, tr0, 'r', label = 'low rate')\n",
    "    # plt.plot(time_aligned_stim, np.nanmean(Xt[:, :, lr_trs],  axis = (1, 2)), 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_stim, np.nanmean(Xt[:, :, hr_trs],  axis = (1, 2)), 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to stimulus onset (ms)')\n",
    "    plt.title('Population average')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # goTone-aligned projections and raw average\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtg_w[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtg_w[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtg_w[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtg_w[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_go, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_go, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_go, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_go, tr0, 'r', label = 'low rate')\n",
    "    # tr1=np.nanmean(Xtc_w[:, hr_trs],  axis = 1)\n",
    "    # tr0=np.nanmean(Xtc_w[:, lr_trs],  axis = 1)\n",
    "    # plt.plot(time_aligned_1stSide, tr1, 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_1stSide, tr0, 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to go tone (ms)')\n",
    "    plt.title('Projections onto classifier weights')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    a1 = np.nanmean(Xtg[:, :, hr_trs],  axis=1) # frames x trials\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xtg[:, :, lr_trs],  axis=1) # frames x trials\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_go, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_go, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_go, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_go, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to go tone (ms)')\n",
    "    plt.title('Population average')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # choice-aligned projections and raw average\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtc_w[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtc_w[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtc_w[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtc_w[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_1stSide, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_1stSide, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_1stSide, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_1stSide, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to 1st choice (ms)')\n",
    "    plt.title('Projections onto classifier weights')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    a1 = np.nanmean(Xtc[:, :, hr_trs],  axis=1) # frames x trials\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xtc[:, :, lr_trs],  axis=1) # frames x trials\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_1stSide, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_1stSide, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_1stSide, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_1stSide, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to 1st choice (ms)')\n",
    "    plt.title('Population average')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # reward-aligned projections and raw average\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtr_w[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtr_w[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtr_w[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtr_w[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_rew, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_rew, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_rew, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_rew, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to reward (ms)')\n",
    "    plt.title('Projections onto classifier weights')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    a1 = np.nanmean(Xtr[:, :, hr_trs],  axis=1) # frames x trials\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xtr[:, :, lr_trs],  axis=1) # frames x trials\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_rew, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_rew, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_rew, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_rew, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to reward (ms)')\n",
    "    plt.title('Population average')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # incommitResp-aligned projections and raw average\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtp_w[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtp_w[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtp_w[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtp_w[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_incorrResp, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_incorrResp, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_incorrResp, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_incorrResp, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to incorrResp (ms)')\n",
    "    plt.title('Projections onto classifier weights')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    a1 = np.nanmean(Xtp[:, :, hr_trs],  axis=1) # frames x trials\n",
    "    tr1 = np.nanmean(a1,  axis = 1)\n",
    "    tr1_se = np.nanstd(a1,  axis = 1) / np.sqrt(numTrials);\n",
    "    a0 = np.nanmean(Xtp[:, :, lr_trs],  axis=1) # frames x trials\n",
    "    tr0 = np.nanmean(a0,  axis = 1)\n",
    "    tr0_se = np.nanstd(a0,  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_incorrResp, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_incorrResp, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_incorrResp, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_incorrResp, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to incorrResp (ms)')\n",
    "    plt.title('Population average')\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "b491379f-ef3f-4a82-924a-905110330175"
    }
   },
   "source": [
    "## Classification accuracy at each time point\n",
    "    Same trials that went into projection traces will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "702f2432-bd68-4534-a16f-acae33db3026"
    }
   },
   "outputs": [],
   "source": [
    "# perClassCorr_t = [];\n",
    "# corrClass = np.ones((T, numTrials)) + np.nan # frames x trials\n",
    "\n",
    "# temp = Xti # for trial-history you may want to use this:\n",
    "temp = Xt # stimulus-aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "nnf, nnu, nnt = temp.shape\n",
    "corrClass = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "\n",
    "# if trs4project!='trained':  # onlyTrainedTrs==0: \n",
    "#     temp = temp[:,:,~trsExcluded] # make sure it has same size as Y, you need this for svm.predict below. \n",
    "for t in range(T):\n",
    "    trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "    corrFract = 1 - abs(linear_svm.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "    # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "    corrClass[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "    # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot class accuracy\n",
    "if doPlots:\n",
    "    a = np.mean(corrClass, axis=1)*100 \n",
    "    s = np.std(corrClass, axis=1)*100 /np.sqrt(numTrials);\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.fill_between(time_aligned_stim, a-s, a+s, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, a, 'r')\n",
    "    plt.xlabel('time since stimulus onset (ms)')\n",
    "    plt.ylabel('classification accuracy (%)')\n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Excitatory and Inhibitory Neurons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare weights of inhibitory and excitatory neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doPlots and neuronType==2:\n",
    "    w_inh = w[inhRois==1]\n",
    "    w_exc = w[inhRois==0]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(w_inh, label = 'inhibit')\n",
    "    plt.plot(w_exc, label = 'excit')\n",
    "    plt.ylabel('Weights')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, .7))    \n",
    "\n",
    "    h, p = stats.ttest_ind(w_inh, w_exc)\n",
    "    \n",
    "    plt.title('mean(abs(w)): inhibit = %.3f  excit = %.3f\\np-val_2tailed (inhibit vs excit weights) = %.2f ' %(np.mean(abs(w_inh)), np.mean(abs(w_exc)), p))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    print '\\nmean(abs(w)): inhibit = %.3f  excit = %.3f' %(np.mean(abs(w_inh)), np.mean(abs(w_exc)))\n",
    "    print 'p-val_2tailed (inhibit vs excit weights) = %.2f' %p\n",
    "    # p_tl = ttest2(w_inh, w_exc, tail='left')\n",
    "    # print 'p-val_left_tailed (inhibit vs excit weights) = %.2f' %p_tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare projections for when only excitatory vs. only inhibitory neurons are contributing to the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if doPlots and neuronType==2:\n",
    "\n",
    "    # Set weights of excit neurons to 0 and project the traces onto this new decoder that cares only about inhibitory neurons.\n",
    "\n",
    "    ww = w+0;\n",
    "    ww[inhRois==0] = 0\n",
    "\n",
    "    w_normalized = ww/sci.linalg.norm(w);\n",
    "\n",
    "    # stim-aligned traces\n",
    "    # XtN_w = np.dot(Xt_N, w_normalized);\n",
    "    # Xt_w = np.reshape(XtN_w, (T,C), order='F');\n",
    "    # I think below can replace above, test it....\n",
    "    XtN_w = np.dot(Xtsa_N, w_normalized);\n",
    "    Xt_w_i = np.reshape(XtN_w, (Tsa,Csa), order='F');\n",
    "\n",
    "\n",
    "    # goTone-aligned \n",
    "    XtgN_w = np.dot(Xtg_N, w_normalized);\n",
    "    Xtg_w_i = np.reshape(XtgN_w, (Tg,Cg), order='F');\n",
    "\n",
    "    # choice-aligned \n",
    "    XtcN_w = np.dot(Xtc_N, w_normalized);\n",
    "    Xtc_w_i = np.reshape(XtcN_w, (Tc,Cc), order='F');\n",
    "\n",
    "    # reward-aligned \n",
    "    XtrN_w = np.dot(Xtr_N, w_normalized);\n",
    "    Xtr_w_i = np.reshape(XtrN_w, (Tr,Cr), order='F');\n",
    "\n",
    "    # incommitResp-aligned \n",
    "    XtpN_w = np.dot(Xtp_N, w_normalized);\n",
    "    Xtp_w_i = np.reshape(XtpN_w, (Tp,Cp), order='F');\n",
    "\n",
    "    # initTone-aligned \n",
    "    XtiN_w = np.dot(Xti_N, w_normalized);\n",
    "    Xti_w_i = np.reshape(XtiN_w, (Ti,Ci), order='F');\n",
    "\n",
    "\n",
    "\n",
    "    # Set weights of inhibit neurons to 0 and project the traces onto this new decoder that cares only about the excitatory neurons.\n",
    "\n",
    "    ww = w+0;\n",
    "    ww[inhRois==1] = 0\n",
    "\n",
    "    w_normalized = ww/sci.linalg.norm(w);\n",
    "\n",
    "    # stim-aligned traces\n",
    "    # XtN_w = np.dot(Xt_N, w_normalized);\n",
    "    # Xt_w = np.reshape(XtN_w, (T,C), order='F');\n",
    "    # I think below can replace above, test it....\n",
    "    XtN_w = np.dot(Xtsa_N, w_normalized);\n",
    "    Xt_w_e = np.reshape(XtN_w, (Tsa,Csa), order='F');\n",
    "\n",
    "\n",
    "    # goTone-aligned \n",
    "    XtgN_w = np.dot(Xtg_N, w_normalized);\n",
    "    Xtg_w_e = np.reshape(XtgN_w, (Tg,Cg), order='F');\n",
    "\n",
    "    # choice-aligned \n",
    "    XtcN_w = np.dot(Xtc_N, w_normalized);\n",
    "    Xtc_w_e = np.reshape(XtcN_w, (Tc,Cc), order='F');\n",
    "\n",
    "    # reward-aligned \n",
    "    XtrN_w = np.dot(Xtr_N, w_normalized);\n",
    "    Xtr_w_e = np.reshape(XtrN_w, (Tr,Cr), order='F');\n",
    "\n",
    "    # incommitResp-aligned \n",
    "    XtpN_w = np.dot(Xtp_N, w_normalized);\n",
    "    Xtp_w_e = np.reshape(XtpN_w, (Tp,Cp), order='F');\n",
    "\n",
    "    # initTone-aligned \n",
    "    XtiN_w = np.dot(Xti_N, w_normalized);\n",
    "    Xti_w_e = np.reshape(XtiN_w, (Ti,Ci), order='F');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the excitatory vs inhibitory projections\n",
    "if doPlots and neuronType==2:\n",
    "\n",
    "    # window of training (ep)\n",
    "    # win = (ep-eventI)*frameLength\n",
    "\n",
    "    # init-aligned projections: excit vs inhibit\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xti_w_e[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xti_w_e[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xti_w_e[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xti_w_e[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_init, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_init, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_init, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_init, tr0, 'r', label = 'low rate')\n",
    "    # tr1=np.nanmean(Xtc_w[:, hr_trs],  axis = 1)\n",
    "    # tr0=np.nanmean(Xtc_w[:, lr_trs],  axis = 1)\n",
    "    # plt.plot(time_aligned_1stSide, tr1, 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_1stSide, tr0, 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to init tone (ms)')\n",
    "    plt.title('Excitatory projections')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    tr1 = np.nanmean(Xti_w_i[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xti_w_i[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xti_w_i[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xti_w_i[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_init, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_init, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_init, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_init, tr0, 'r', label = 'low rate')\n",
    "    # tr1=np.nanmean(Xtc_w[:, hr_trs],  axis = 1)\n",
    "    # tr0=np.nanmean(Xtc_w[:, lr_trs],  axis = 1)\n",
    "    # plt.plot(time_aligned_1stSide, tr1, 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_1stSide, tr0, 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to init tone (ms)')\n",
    "    plt.title('Inhibitory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # stim-aligned projections: excit vs inhibit\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xt_w_e[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xt_w_e[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xt_w_e[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xt_w_e[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_stim, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_stim, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_stim, tr0, 'r', label = 'low rate')\n",
    "    mn = np.concatenate([tr1,tr0]).min()\n",
    "    mx = np.concatenate([tr1,tr0]).max()\n",
    "    plt.plot([win[0], win[0]], [mn, mx], 'g-.') # mark the begining and end of training window\n",
    "    plt.plot([win[-1], win[-1]], [mn, mx], 'g-.')\n",
    "    plt.xlabel('time aligned to stimulus onset (ms)')\n",
    "    plt.title('Excitatory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    tr1 = np.nanmean(Xt_w_i[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xt_w_i[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xt_w_i[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xt_w_i[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_stim, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_stim, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_stim, tr0, 'r', label = 'low rate')\n",
    "    mn = np.concatenate([tr1,tr0]).min()\n",
    "    mx = np.concatenate([tr1,tr0]).max()\n",
    "    plt.plot([win[0], win[0]], [mn, mx], 'g-.') # mark the begining and end of training window\n",
    "    plt.plot([win[-1], win[-1]], [mn, mx], 'g-.')\n",
    "    plt.xlabel('time aligned to stimulus onset (ms)')\n",
    "    plt.title('Inhibitory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # goTone-aligned projections: excit vs inhibit\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtg_w_e[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtg_w_e[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtg_w_e[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtg_w_e[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_go, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_go, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_go, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_go, tr0, 'r', label = 'low rate')\n",
    "    # tr1=np.nanmean(Xtc_w[:, hr_trs],  axis = 1)\n",
    "    # tr0=np.nanmean(Xtc_w[:, lr_trs],  axis = 1)\n",
    "    # plt.plot(time_aligned_1stSide, tr1, 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_1stSide, tr0, 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to go tone (ms)')\n",
    "    plt.title('Excitatory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    tr1 = np.nanmean(Xtg_w_i[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtg_w_i[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtg_w_i[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtg_w_i[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_go, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_go, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_go, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_go, tr0, 'r', label = 'low rate')\n",
    "    # tr1=np.nanmean(Xtc_w[:, hr_trs],  axis = 1)\n",
    "    # tr0=np.nanmean(Xtc_w[:, lr_trs],  axis = 1)\n",
    "    # plt.plot(time_aligned_1stSide, tr1, 'r', label = 'high rate')\n",
    "    # plt.plot(time_aligned_1stSide, tr0, 'b', label = 'low rate')\n",
    "    plt.xlabel('time aligned to go tone (ms)')\n",
    "    plt.title('Inhibitory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "    # choice-aligned projections: excit vs inhibit\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtc_w_e[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtc_w_e[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtc_w_e[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtc_w_e[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_1stSide, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_1stSide, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_1stSide, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_1stSide, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to 1st choice (ms)')\n",
    "    plt.title('Excitatory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    tr1 = np.nanmean(Xtc_w_i[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtc_w_i[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtc_w_i[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtc_w_i[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_1stSide, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_1stSide, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_1stSide, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_1stSide, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to 1st choice (ms)')\n",
    "    plt.title('Inhibitory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # reward-aligned projections: excit vs inhibit\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtr_w_e[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtr_w_e[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtr_w_e[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtr_w_e[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_rew, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_rew, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_rew, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_rew, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to reward (ms)')\n",
    "    plt.title('Excitatory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    tr1 = np.nanmean(Xtr_w_i[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtr_w_i[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtr_w_i[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtr_w_i[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_rew, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_rew, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_rew, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_rew, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to reward (ms)')\n",
    "    plt.title('Inhibitory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # incommitResp-aligned projections: excit vs inhibit\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    tr1 = np.nanmean(Xtp_w_e[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtp_w_e[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtp_w_e[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtp_w_e[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_incorrResp, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_incorrResp, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_incorrResp, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_incorrResp, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to incorrResp (ms)')\n",
    "    plt.title('Excitatory projections')\n",
    "    # plt.legend()\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    tr1 = np.nanmean(Xtp_w_i[:, hr_trs],  axis = 1)\n",
    "    tr1_se = np.nanstd(Xtp_w_i[:, hr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    tr0 = np.nanmean(Xtp_w_i[:, lr_trs],  axis = 1)\n",
    "    tr0_se = np.nanstd(Xtp_w_i[:, lr_trs],  axis = 1) / np.sqrt(numTrials);\n",
    "    plt.fill_between(time_aligned_incorrResp, tr1-tr1_se, tr1+tr1_se, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.fill_between(time_aligned_incorrResp, tr0-tr0_se, tr0+tr0_se, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_incorrResp, tr1, 'b', label = 'high rate')\n",
    "    plt.plot(time_aligned_incorrResp, tr0, 'r', label = 'low rate')\n",
    "    plt.xlabel('time aligned to incorrResp (ms)')\n",
    "    plt.title('Inhibitory projections')\n",
    "    # plt.legend()\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute classification error of training and testing dataset for the following cases:\n",
    "    1) All inhibitory neurons contribute to the decoder\n",
    "    2) N excitatory neurons contribute, where n = number of inhibitory neurons. \n",
    "    3) All excitatory neurons contribute\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if neuronType==2:\n",
    "    \n",
    "    Xinh = X[:, inhRois==1]\n",
    "    XallExc = X[:, inhRois==0]\n",
    "    lenInh = (inhRois==1).sum()\n",
    "    excI = np.argwhere(inhRois==0)  \n",
    "    \n",
    "    numShuffles = 100 \n",
    "    numShufflesExc = 10 # we choose n random excitator neurons, how many times to do this?\n",
    "\n",
    "    summary_data_inh = [];\n",
    "    summary_shfl_inh = [];\n",
    "    summary_data_allExc = [];\n",
    "    summary_shfl_allExc = [];\n",
    "    summary_data_exc = [];\n",
    "    summary_shfl_exc = [];\n",
    "    perClassErrorTrain_data_inh = [];\n",
    "    perClassErrorTest_data_inh = []\n",
    "    perClassErrorTrain_shfl_inh = [];\n",
    "    perClassErrorTest_shfl_inh = [];\n",
    "    w_data_inh = []\n",
    "    b_data_inh = []\n",
    "    w_shfl_inh = []\n",
    "    b_shfl_inh = []\n",
    "    perClassErrorTrain_data_allExc = [];\n",
    "    perClassErrorTest_data_allExc = []\n",
    "    perClassErrorTrain_shfl_allExc = [];\n",
    "    perClassErrorTest_shfl_allExc = [];\n",
    "    w_data_allExc = []\n",
    "    b_data_allExc = []\n",
    "    w_shfl_allExc = []\n",
    "    b_shfl_allExc = []\n",
    "    perClassErrorTrain_data_exc = [];\n",
    "    perClassErrorTest_data_exc = []\n",
    "    perClassErrorTrain_shfl_exc = [];\n",
    "    perClassErrorTest_shfl_exc = [];\n",
    "    w_data_exc = []\n",
    "    b_data_exc = []\n",
    "    w_shfl_exc = []\n",
    "    b_shfl_exc = []\n",
    "\n",
    "    permIxsList = [];\n",
    "    counter = 0    \n",
    "    \n",
    "    for i in range(numShuffles):\n",
    "        # permIxs = rng.permutation(numTrials);\n",
    "        permIxs = rng.permutation(numDataPoints);\n",
    "        permIxsList.append(permIxs);    \n",
    "\n",
    "        if regType == 'l1':\n",
    "            # all inh            \n",
    "            summary_data_inh.append(crossValidateModel(Xinh, Y, linearSVM, kfold = kfold, l1 = cbest))\n",
    "            summary_shfl_inh.append(crossValidateModel(Xinh, Y[permIxs], linearSVM, kfold = kfold, l1 = cbest))\n",
    "\n",
    "\n",
    "            # all exc            \n",
    "            summary_data_allExc.append(crossValidateModel(XallExc, Y, linearSVM, kfold = kfold, l1 = cbest))\n",
    "            summary_shfl_allExc.append(crossValidateModel(XallExc, Y[permIxs], linearSVM, kfold = kfold, l1 = cbest))\n",
    "\n",
    "\n",
    "            # n exc (n = number of inh)        \n",
    "            # select n random exc (n = number of inh)\n",
    "            for ii in range(numShufflesExc):            \n",
    "                en = rng.permutation(excI)[0:lenInh].squeeze() # n randomly selected exc neurons.\n",
    "                Xexc = X[:, en]\n",
    "                summary_data_exc.append(crossValidateModel(Xexc, Y, linearSVM, kfold = kfold, l1 = cbest)) # summary_data_exc is of size numShuffles x numShufflesExc\n",
    "                summary_shfl_exc.append(crossValidateModel(Xexc, Y[permIxs], linearSVM, kfold = kfold, l1 = cbest))\n",
    "\n",
    "                counter = counter+1\n",
    "    #             print en\n",
    "#             print counter\n",
    "            inds = np.arange(counter-numShufflesExc,counter)\n",
    "#             print inds\n",
    "\n",
    "\n",
    "        elif regType == 'l2':\n",
    "            # all inh\n",
    "            summary_data_inh.append(crossValidateModel(Xinh, Y, linearSVM, kfold = kfold, l2 = cbest))\n",
    "            summary_shfl_inh.append(crossValidateModel(Xinh, Y[permIxs], linearSVM, kfold = kfold, l2 = cbest))\n",
    "\n",
    "\n",
    "            # all exc\n",
    "            summary_data_allExc.append(crossValidateModel(XallExc, Y, linearSVM, kfold = kfold, l2 = cbest))\n",
    "            summary_shfl_allExc.append(crossValidateModel(XallExc, Y[permIxs], linearSVM, kfold = kfold, l2 = cbest))\n",
    "\n",
    "\n",
    "            # n exc (n = number of inh)     \n",
    "            for ii in range(numShufflesExc):\n",
    "                Xexc = X[:, en]\n",
    "                summary_data_exc.append(crossValidateModel(Xexc, Y, linearSVM, kfold = kfold, l2 = cbest))\n",
    "                summary_shfl_exc.append(crossValidateModel(Xexc, Y[permIxs], linearSVM, kfold = kfold, l2 = cbest))\n",
    "\n",
    "                counter = counter+1\n",
    "#             print counter\n",
    "            inds = np.arange(counter-numShufflesExc,counter)\n",
    "#             print inds\n",
    "\n",
    "\n",
    "\n",
    "        # inh        \n",
    "        perClassErrorTrain_data_inh.append(summary_data_inh[i].perClassErrorTrain);\n",
    "        perClassErrorTest_data_inh.append(summary_data_inh[i].perClassErrorTest);\n",
    "        w_data_inh.append(np.squeeze(summary_data_inh[i].model.coef_));\n",
    "        b_data_inh.append(summary_data_inh[i].model.intercept_);\n",
    "\n",
    "        perClassErrorTrain_shfl_inh.append(summary_shfl_inh[i].perClassErrorTrain);\n",
    "        perClassErrorTest_shfl_inh.append(summary_shfl_inh[i].perClassErrorTest);\n",
    "        w_shfl_inh.append(np.squeeze(summary_shfl_inh[i].model.coef_));\n",
    "        b_shfl_inh.append(summary_shfl_inh[i].model.intercept_);\n",
    "\n",
    "        # all exc\n",
    "        perClassErrorTrain_data_allExc.append(summary_data_allExc[i].perClassErrorTrain);\n",
    "        perClassErrorTest_data_allExc.append(summary_data_allExc[i].perClassErrorTest);\n",
    "        w_data_allExc.append(np.squeeze(summary_data_allExc[i].model.coef_));\n",
    "        b_data_allExc.append(summary_data_allExc[i].model.intercept_);\n",
    "\n",
    "        perClassErrorTrain_shfl_allExc.append(summary_shfl_allExc[i].perClassErrorTrain);\n",
    "        perClassErrorTest_shfl_allExc.append(summary_shfl_allExc[i].perClassErrorTest);\n",
    "        w_shfl_allExc.append(np.squeeze(summary_shfl_allExc[i].model.coef_));\n",
    "        b_shfl_allExc.append(summary_shfl_allExc[i].model.intercept_);\n",
    "\n",
    "        # n exc\n",
    "        perClassErrorTrain_data_exc.append(np.mean([summary_data_exc[inds[j]].perClassErrorTrain for j in range(len(inds))], axis=0)); # append the average of shuffles for selecting n exc neurons);\n",
    "        perClassErrorTest_data_exc.append(np.mean([summary_data_exc[inds[j]].perClassErrorTest for j in range(len(inds))], axis=0));\n",
    "        w_data_exc.append(np.mean([(summary_data_exc[inds[j]].model.coef_).squeeze() for j in range(len(inds))], axis=0));\n",
    "        b_data_exc.append(np.mean([(summary_data_exc[inds[j]].model.intercept_).squeeze() for j in range(len(inds))], axis=0));\n",
    "\n",
    "        perClassErrorTrain_shfl_exc.append(np.mean([summary_shfl_exc[inds[j]].perClassErrorTrain for j in range(len(inds))], axis=0)) # average across shuffles for selecting n exc neurons);\n",
    "        perClassErrorTest_shfl_exc.append(np.mean([summary_shfl_exc[inds[j]].perClassErrorTest for j in range(len(inds))], axis=0));\n",
    "        w_shfl_exc.append(np.mean([(summary_shfl_exc[inds[j]].model.coef_).squeeze() for j in range(len(inds))], axis=0));\n",
    "        b_shfl_exc.append(np.mean([(summary_shfl_exc[inds[j]].model.intercept_).squeeze() for j in range(len(inds))], axis=0));\n",
    "\n",
    "\n",
    "    #     # n exc\n",
    "    #     perClassErrorTrain_data_exc.append(summary_data_exc[i].perClassErrorTrain);\n",
    "    #     perClassErrorTest_data_exc.append(summary_data_exc[i].perClassErrorTest);\n",
    "    #     w_data_exc.append(np.squeeze(summary_data_exc[i].model.coef_));\n",
    "    #     b_data_exc.append(summary_data_exc[i].model.intercept_);\n",
    "\n",
    "    #     perClassErrorTrain_shfl_exc.append(summary_shfl_exc[i].perClassErrorTrain);\n",
    "    #     perClassErrorTest_shfl_exc.append(summary_shfl_exc[i].perClassErrorTest);\n",
    "    #     w_shfl_exc.append(np.squeeze(summary_shfl_exc[i].model.coef_));\n",
    "    #     b_shfl_exc.append(summary_shfl_exc[i].model.intercept_);\n",
    "\n",
    "\n",
    "    # Simply get all values of all shuffles instead of appending like above    \n",
    "    # perClassErrorTrain_data_exc = [summary_data_exc[i].perClassErrorTrain for i in range(np.shape(summary_data_exc)[0])]\n",
    "    # perClassErrorTest_data_exc = [summary_data_exc[i].perClassErrorTest for i in range(np.shape(summary_data_exc)[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot histograms of class error for training and testing datasets\n",
    "\n",
    "if neuronType==2:\n",
    "    \n",
    "    # inh\n",
    "    print 'All inhibitory neurons'\n",
    "    pd = perClassErrorTrain_data_inh\n",
    "    ps = perClassErrorTrain_shfl_inh\n",
    "    pd0 = perClassErrorTest_data_inh\n",
    "    ps0 = perClassErrorTest_shfl_inh\n",
    "    pvalueTrain = ttest2(pd, ps, tail = 'left');\n",
    "    pvalueTest = ttest2(pd0, ps0, tail = 'left');\n",
    "\n",
    "    print '\\tTraining error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(pd), np.mean(ps), pvalueTrain)\n",
    "    print '\\tTesting error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(pd0), np.mean(ps0), pvalueTest)\n",
    "\n",
    "\n",
    "    # Plot the histograms\n",
    "    if doPlots:\n",
    "        binEvery = 3; # bin width\n",
    "\n",
    "        plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.hist(pd, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "        plt.hist(ps, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "        plt.xlabel('Training classification error (%)')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('Inhibitory neurons\\nMean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(pd), np.mean(ps), pvalueTrain), fontsize = 10)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.hist(pd0, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "        plt.hist(ps0, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "        plt.legend()\n",
    "        plt.xlabel('Testing classification error (%)')\n",
    "        plt.title('Mean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(pd0), np.mean(ps0), pvalueTest), fontsize = 10)\n",
    "        plt.ylabel('count')\n",
    "        plt.tight_layout(pad=0.4, w_pad=1.5, h_pad=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # n exc    \n",
    "    print 'Only n of the excitory neurons'\n",
    "    pd = perClassErrorTrain_data_exc\n",
    "    ps = perClassErrorTrain_shfl_exc\n",
    "    pd0 = perClassErrorTest_data_exc\n",
    "    ps0 = perClassErrorTest_shfl_exc\n",
    "    pvalueTrain = ttest2(pd, ps, tail = 'left');\n",
    "    pvalueTest = ttest2(pd0, ps0, tail = 'left');\n",
    "\n",
    "    print '\\tTraining error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(pd), np.mean(ps), pvalueTrain)\n",
    "    print '\\tTesting error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(pd0), np.mean(ps0), pvalueTest)\n",
    "\n",
    "\n",
    "    # Plot the histograms\n",
    "    if doPlots:\n",
    "        binEvery = 3; # bin width\n",
    "\n",
    "        plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.hist(pd, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "        plt.hist(ps, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "        plt.xlabel('Training classification error (%)')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('n excitatory neurons\\nMean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(pd), np.mean(ps), pvalueTrain), fontsize = 10)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.hist(pd0, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "        plt.hist(ps0, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "        plt.legend()\n",
    "        plt.xlabel('Testing classification error (%)')\n",
    "        plt.title('Mean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(pd0), np.mean(ps0), pvalueTest), fontsize = 10)\n",
    "        plt.ylabel('count')\n",
    "        plt.tight_layout(pad=0.4, w_pad=1.5, h_pad=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # all exc    \n",
    "    print 'All excitatory neurons'\n",
    "    pd = perClassErrorTrain_data_allExc\n",
    "    ps = perClassErrorTrain_shfl_allExc\n",
    "    pd0 = perClassErrorTest_data_allExc\n",
    "    ps0 = perClassErrorTest_shfl_allExc\n",
    "    pvalueTrain = ttest2(pd, ps, tail = 'left');\n",
    "    pvalueTest = ttest2(pd0, ps0, tail = 'left');\n",
    "\n",
    "    print '\\tTraining error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(pd), np.mean(ps), pvalueTrain)\n",
    "    print '\\tTesting error: Mean actual: %.2f%%, Mean shuffled: %.2f%%, p-value = %.2f' %(np.mean(pd0), np.mean(ps0), pvalueTest)\n",
    "\n",
    "\n",
    "    # Plot the histograms\n",
    "    if doPlots:\n",
    "        binEvery = 3; # bin width\n",
    "\n",
    "        plt.figure()\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.hist(pd, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "        plt.hist(ps, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "        plt.xlabel('Training classification error (%)')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('All excitatory neurons\\nMean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(pd), np.mean(ps), pvalueTrain), fontsize = 10)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.hist(pd0, np.arange(0,100,binEvery), color = 'k', label = 'data');\n",
    "        plt.hist(ps0, np.arange(0,100,binEvery), color = 'k', alpha=.5, label = 'shuffled');\n",
    "        plt.legend()\n",
    "        plt.xlabel('Testing classification error (%)')\n",
    "        plt.title('Mean data: %.2f %%, Mean shuffled: %.2f %%\\n p-value = %.2f' %(np.mean(pd0), np.mean(ps0), pvalueTest), fontsize = 10)\n",
    "        plt.ylabel('count')\n",
    "        plt.tight_layout(pad=0.4, w_pad=1.5, h_pad=1.0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set correct classification traces for the following cases:\n",
    "    1) All inhibitory neurons contribute to the decoder\n",
    "    2) N excitatory neurons contribute, where n = number of inhibitory neurons. \n",
    "    3) All excitatory neurons contribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if neuronType==2:\n",
    "    \n",
    "    Xinh = X[:, inhRois==1]\n",
    "    XallExc = X[:, inhRois==0]\n",
    "    lenInh = (inhRois==1).sum()\n",
    "    excI = np.argwhere(inhRois==0)  \n",
    "\n",
    "    \n",
    "    #%% Set correct classification traces for inh\n",
    "    linear_svm_inh = copy.deepcopy(linear_svm_0)\n",
    "    linear_svm_inh.fit(Xinh, Y)\n",
    "    # w = np.squeeze(linear_svm.coef_);\n",
    "    # b = linear_svm.intercept_;\n",
    "    # trainE_inh = abs(linear_svm_inh.predict(Xinh)-Y.astype('float')).sum()/len(Y)*100;\n",
    "    # trainE_inh\n",
    "\n",
    "    tempInh = Xt[:,inhRois==1,:] # stimulus-aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "    temp = tempInh\n",
    "    linear_svm_now = linear_svm_inh\n",
    "\n",
    "    nnf, nnu, nnt = temp.shape\n",
    "    corrClass_inh = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "\n",
    "    for t in range(T):\n",
    "        trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "        corrFract = 1 - abs(linear_svm_now.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        corrClass_inh[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "        # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    \n",
    "\n",
    "#     corrClass_inh = np.mean(corrClass2, axis = 1) # frames x 1. Average correct class across trials for each shuffle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%% Set correct classification traces for all exc\n",
    "    linear_svm_allExc = copy.deepcopy(linear_svm_0)\n",
    "    linear_svm_allExc.fit(XallExc, Y)\n",
    "    # w = np.squeeze(linear_svm.coef_);\n",
    "    # b = linear_svm.intercept_;\n",
    "    # trainE_allExc = abs(linear_svm_allExc.predict(XallExc)-Y.astype('float')).sum()/len(Y)*100;\n",
    "    # trainE_allExc\n",
    "\n",
    "    tempAllExc = Xt[:,inhRois==0,:] # stimulus-aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "    temp = tempAllExc\n",
    "    linear_svm_now = linear_svm_allExc\n",
    "\n",
    "    nnf, nnu, nnt = temp.shape\n",
    "    corrClass_allExc = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "\n",
    "    for t in range(T):\n",
    "        trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "        corrFract = 1 - abs(linear_svm_now.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        corrClass_allExc[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "        # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    \n",
    "\n",
    "#     corrClass_allExc = np.mean(corrClass2, axis = 1) # frames x 1. Average correct class across trials for each shuffle.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #%% Set correct classification traces for n exc\n",
    "    numShufflesExc = 10 # we choose n random excitator neurons, how many times to do this?\n",
    "    corrClass_exc = []\n",
    "    for ii in range(numShufflesExc):            \n",
    "        en = rng.permutation(excI)[0:lenInh].squeeze() # n randomly selected exc neurons.\n",
    "        Xexc = X[:, en]\n",
    "\n",
    "        linear_svm_exc = copy.deepcopy(linear_svm_0)\n",
    "        linear_svm_exc.fit(Xexc, Y)\n",
    "        # w = np.squeeze(linear_svm.coef_);\n",
    "        # b = linear_svm.intercept_;\n",
    "        # trainE_exc = abs(linear_svm_exc.predict(Xexc)-Y.astype('float')).sum()/len(Y)*100;\n",
    "        # trainE_exc\n",
    "\n",
    "        tempExc = Xt[:,en,:] # stimulus-aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "        temp = tempExc\n",
    "        linear_svm_now = linear_svm_exc\n",
    "\n",
    "        nnf, nnu, nnt = temp.shape\n",
    "        corrClass2 = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "\n",
    "        for t in range(T):\n",
    "            trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "            corrFract = 1 - abs(linear_svm_now.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "            # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "            corrClass2[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "            # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    \n",
    "\n",
    "        corrClass_exc.append(corrClass2) # numShuffExc x frames x trials. Average correct class across trials for each shuffle.           \n",
    "    s1, s2, s3 = np.shape(corrClass_exc)\n",
    "    a = np.reshape(np.transpose(corrClass_exc, (1,0,2)), (s2,s1*s3), order = 'F') # frames x (numShuffExc x trials)\n",
    "    corrClass_exc = a\n",
    "    \n",
    "#         corrClass_exc.append(np.mean(corrClass2, axis = 1)) # numShuffExc x frames. Average correct class across trials for each shuffle.    \n",
    "#     corrClass_exc = np.transpose(corrClass_exc) # frames x numShuffExc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot correct classification traces for the following cases:\n",
    "#     1) All inhibitory neurons contribute to the decoder\n",
    "#     2) N excitatory neurons contribute, where n = number of inhibitory neurons. \n",
    "#     3) All excitatory neurons contribute\n",
    "    \n",
    "if doPlots and neuronType==2:\n",
    "    \n",
    "    plt.figure()   \n",
    "#     plt.subplot(1,2,1)\n",
    "    \n",
    "    # Plot corrClass for all excitatory neurons\n",
    "    a1 = np.mean(corrClass_allExc, axis=1)*100\n",
    "    s1 = np.std(corrClass_allExc, axis=1)*100 /np.sqrt(corrClass_allExc.shape[1])    \n",
    "    \n",
    "    plt.fill_between(time_aligned_stim, a1-s1, a1+s1, alpha=0.3, edgecolor='m', facecolor='m')\n",
    "    plt.plot(time_aligned_stim, a1, 'm', label = 'all exc')\n",
    "    plt.xlabel('Time since stimulus onset (ms)')\n",
    "#     plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot corrClass for n excitatory neurons\n",
    "    a1 = np.mean(corrClass_exc, axis=1)*100\n",
    "    s1 = np.std(corrClass_exc, axis=1)*100 /np.sqrt(corrClass_exc.shape[1])    \n",
    "    \n",
    "    plt.fill_between(time_aligned_stim, a1-s1, a1+s1, alpha=0.3, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, a1, 'r', label = 'n exc')\n",
    "    plt.xlabel('Time since stimulus onset (ms)')\n",
    "#     plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot corrClass for all inhibitory neurons\n",
    "    a1 = np.mean(corrClass_inh, axis=1)*100\n",
    "    s1 = np.std(corrClass_inh, axis=1)*100 /np.sqrt(corrClass_inh.shape[1])    \n",
    "    \n",
    "    plt.fill_between(time_aligned_stim, a1-s1, a1+s1, alpha=0.3, edgecolor='b', facecolor='b')\n",
    "    plt.plot(time_aligned_stim, a1, 'b', label = 'all inh')\n",
    "    plt.xlabel('Time since stimulus onset (ms)')\n",
    "#     plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "    \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, .7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way of doing the analysis above: compare prediction error when setting weights of excitatory neurons to 0 with when setting weights of inhibitory neurons to 0\n",
    "    When setting weights of excit neurons to 0, we only do it for n excit neurons, where n = number of inhibitory neurons. This is to  control for the difference in the number of excit and inhibit neurons.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if neuronType==2:\n",
    "    \n",
    "    # compute prediction error when all neurons are included.\n",
    "    linear_svm = copy.deepcopy(linear_svm_0)\n",
    "    trainE = abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100;\n",
    "    \n",
    "    \n",
    "    # compute prediction error when setting weights of n excitatory neurons to 0, where n = number of inhibitory neurons\n",
    "    \n",
    "    numShuffles = 100\n",
    "    lenInh = (inhRois==1).sum()\n",
    "    train_err_exc0 = []    \n",
    "    excI = np.argwhere(inhRois==0)    \n",
    "\n",
    "    corrClass_exc0 = np.full([Xt.shape[0], numShuffles], np.nan) # frames x numShuffles. Average correct class across trials for each shuffle when exc weights are set to 0.\n",
    "    \n",
    "    for i in range(numShuffles):\n",
    "        # linear_svm = svm.LinearSVC(C = cbest, loss='squared_hinge', penalty='l1', dual=False)\n",
    "        # linear_svm.fit(X, Y)  \n",
    "        # ww = np.squeeze(linear_svm.coef_);\n",
    "        ww = w+0; # adding 0 is important, otherwise, ww=w will change the value of w as well.\n",
    "        \n",
    "        # set weights of random sets of excit neurons (of size equal to length of inhibit neurons) to 0.\n",
    "        en = rng.permutation(excI)[0:lenInh].squeeze()\n",
    "        ww[en] = 0\n",
    "\n",
    "        linear_svm = copy.deepcopy(linear_svm_0)\n",
    "        linear_svm.coef_ = ww.reshape(1,-1) # here weights of random sets of excit neurons are set to 0 (of size equal to length of inhibit neurons)\n",
    "\n",
    "        # print 'Training error: %.2f %%' %(abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100)\n",
    "        train_err_exc0.append(abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100)\n",
    "\n",
    "\n",
    "        # Set correct classification traces\n",
    "        temp = Xt # stimulus aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "        nnf, nnu, nnt = temp.shape\n",
    "        corrClass2 = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "        # if trs4project!='trained':  # onlyTrainedTrs==0: \n",
    "        #     temp = temp[:,:,~trsExcluded] # make sure it has same size as Y, you need this for svm.predict below. \n",
    "        for t in range(T):\n",
    "            trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "            corrFract = 1 - abs(linear_svm.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "            # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "            corrClass2[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "            # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    \n",
    "        \n",
    "        corrClass_exc0[:,i] = np.mean(corrClass2, axis = 1) # frames x numShuffles. Average correct class across trials for each shuffle.\n",
    "        \n",
    "\n",
    "    # Fraction of change in training error after setting weights of n exc neurons to 0 (n = num inh neurons)   \n",
    "    train_err_exc0_rel2all = (train_err_exc0 - trainE) / trainE    \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    #%% set weights of all excit Ns to 0\n",
    "    ww = w+0;    \n",
    "    ww[inhRois==0] = 0 \n",
    "    linear_svm = copy.deepcopy(linear_svm_0)\n",
    "    linear_svm.coef_ = ww.reshape(1,-1)\n",
    "    train_err_allExc0 = abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100\n",
    "    \n",
    "\n",
    "    # Set correct classification traces\n",
    "    temp = Xt # stimulus aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "    nnf, nnu, nnt = temp.shape\n",
    "    corrClass2 = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "    # if trs4project!='trained':  # onlyTrainedTrs==0: \n",
    "    #     temp = temp[:,:,~trsExcluded] # make sure it has same size as Y, you need this for svm.predict below. \n",
    "    for t in range(T):\n",
    "        trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "        corrFract = 1 - abs(linear_svm.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        corrClass2[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "        # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    \n",
    "\n",
    "    corrClass_allExc0 = np.mean(corrClass2, axis = 1) # frames x 1. Average correct class across trials for each shuffle.\n",
    "    corrClass_allExc0 = corrClass_allExc0[:,np.newaxis] # so it has size (frames x 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #%% compute prediction error when setting weights of inhibitory neurons to 0\n",
    "    numShufflesI = 1; # bc we use all inh neurons (ie no random selection), we don't need to run it multiple times.\n",
    "    train_err_inh0 = []    \n",
    "    inhI = np.argwhere(inhRois==1)    \n",
    "\n",
    "    corrClass_inh0 = np.full([Xt.shape[0], numShufflesI], np.nan) # frames x numShuffles. Average correct class across trials for each shuffle when exc weights are set to 0.\n",
    "    \n",
    "    for i in range(numShufflesI):\n",
    "        # linear_svm = svm.LinearSVC(C = cbest, loss='squared_hinge', penalty='l1', dual=False)\n",
    "        # linear_svm.fit(X, Y)  \n",
    "        # ww = np.squeeze(linear_svm.coef_);\n",
    "        ww = w+0; # adding 0 is important, otherwise, ww=w will change the value of w as well.\n",
    "        \n",
    "        # set weights of random sets of excit neurons (of size equal to length of inhibit neurons) to 0.\n",
    "        en = rng.permutation(inhI).squeeze()\n",
    "        ww[en] = 0\n",
    "\n",
    "        linear_svm = copy.deepcopy(linear_svm_0) # we need to do this bc we changes its coef_ above\n",
    "        linear_svm.coef_ = ww.reshape(1,-1) # here weights of random sets of excit neurons are set to 0 (of size equal to length of inhibit neurons)\n",
    "\n",
    "        # print 'Training error: %.2f %%' %(abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100)\n",
    "        train_err_inh0.append(abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100)\n",
    "\n",
    "\n",
    "        # Set correct classification traces\n",
    "        temp = Xt # stimulus aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "        nnf, nnu, nnt = temp.shape\n",
    "        corrClass2 = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "        # if trs4project!='trained':  # onlyTrainedTrs==0: \n",
    "        #     temp = temp[:,:,~trsExcluded] # make sure it has same size as Y, you need this for svm.predict below. \n",
    "        for t in range(T):\n",
    "            trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "            corrFract = 1 - abs(linear_svm.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "            # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "            corrClass2[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "            # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    \n",
    "        \n",
    "        corrClass_inh0[:,i] = np.mean(corrClass2, axis = 1) # frames x numShuffles. Average correct class across trials for each shuffle.\n",
    "        \n",
    "        \n",
    "        # Fraction of change in training error after setting weights of n exc neurons to 0 (n = num inh neurons)   \n",
    "        train_err_inh0_rel2all = (train_err_inh0 - trainE) / trainE\n",
    "        \n",
    "\n",
    "    linear_svm = copy.deepcopy(linear_svm_0) # go back to the original value\n",
    "        \n",
    "    '''\n",
    "    # linear_svm = svm.LinearSVC(C = cbest, loss='squared_hinge', penalty='l1', dual=False)\n",
    "    # linear_svm.fit(X, Y)  \n",
    "    # ww = np.squeeze(linear_svm.coef_);\n",
    "    ww = w+0;    \n",
    "    ww[inhRois==1] = 0 # set weights of inhibit Ns to 0\n",
    "    \n",
    "    linear_svm = copy.deepcopy(linear_svm_0) # we need to do this bc we changes its coef_ above\n",
    "    linear_svm.coef_ = ww.reshape(1,-1) # here weights of inh neurons are set to 0.\n",
    "    \n",
    "    train_err_inh0 = abs(linear_svm.predict(X)-Y.astype('float')).sum()/len(Y)*100\n",
    "        \n",
    "    # Fraction of change in training error after setting weights of inh neurons to 0\n",
    "    train_err_inh0_rel2all = (train_err_inh0 - trainE) / trainE   \n",
    "    \n",
    "\n",
    "    \n",
    "    # Set correct classification traces\n",
    "    temp = Xt # stimulus aligned traces (I think you should use Xtsa for consistency... u get projections from Xtsa)\n",
    "    nnf, nnu, nnt = temp.shape\n",
    "    corrClass = np.ones((T, nnt)) + np.nan # frames x trials\n",
    "\n",
    "    # if trs4project!='trained':  # onlyTrainedTrs==0: \n",
    "    #     temp = temp[:,:,~trsExcluded] # make sure it has same size as Y, you need this for svm.predict below. \n",
    "    for t in range(T):\n",
    "        trac = np.squeeze(temp[t, :, :]).T # trials x neurons  # stimulus-aligned trace at time t\n",
    "        corrFract = 1 - abs(linear_svm.predict(trac)-choiceVecNow); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        # corrFract = 1 - abs(linear_svm.predict(trac)-Y); # trials x 1 # fraction of correct choice classification using stimulus-aligned neural responses at time t and the trained SVM model linear_svm.\n",
    "        corrClass[t,:] = corrFract # frames x trials % fraction correct classification for all trials\n",
    "        # perClassCorr_t.append(corrFract.mean()*100) # average correct classification across trials for time point t. Same as np.mean(corrClass, axis=1)    \n",
    "\n",
    "    corrClass_inh0 = np.mean(corrClass, axis = 1) # frames x 1. Average correct class across trials for each shuffle when inh weights are set to 0.\n",
    "    corrClass_inh0_std = np.std(corrClass, axis = 1)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #%% P value of fraction changes in training error for exc vs inh weights set to 0. (not sure if better to do it on fraction changes or on the actual training errors?!)\n",
    "#     h, p = stats.ttest_1samp(train_err_exc0_rel2all, train_err_inh0_rel2all)        \n",
    "    h, p = stats.ttest_ind(train_err_exc0, train_err_inh0)        \n",
    "    print '\\nTraining error: \\ninh set to 0 = %.2f%%\\nexcit set to 0 (mean of 100 rounds)) = %.2f%%\\nall exc set to 0 = %.2f%%\\np_val(train_err_exc0: excit vs inhibit set to 0) = %.2f' %(np.mean(train_err_inh0), np.mean(train_err_exc0), np.mean(train_err_allExc0), p)\n",
    "    print '\\nTraining error, change relative to original: \\ninh set to 0 = %.2f\\nexcit set to 0 (mean of 100 rounds)) = %.2f\\np_val(train_err_exc0: excit vs inhibit set to 0) = %.2f' %(np.mean(train_err_inh0_rel2all), np.mean(train_err_exc0_rel2all), p)\n",
    "\n",
    "    \n",
    "    #%% plot the distribution of training error for when random sets of excit weights are set to 0.\n",
    "    if doPlots:\n",
    "        plt.figure()\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.hist(train_err_exc0, np.arange(0,100,3), color = 'k', label = 'data');\n",
    "        plt.xlabel('Train error when setting n excit weights to 0, n=#inhibit neurons')\n",
    "        plt.ylabel('count')\n",
    "        plt.title('Train error:\\nall = %.2f%%\\ninh_zero = %.2f%%; excit_zero = %.2f%%; allExcit_zero = %.2f%%\\np_val = %.2f' %(trainE, np.mean(train_err_inh0), np.mean(train_err_exc0), np.mean(train_err_allExc0), p))\n",
    "        plt.tight_layout#(pad=0.4, w_pad=1.5, h_pad=1.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot changes in class accuracy after setting exc w or inh w to 0 (relative to the original class accuracy).\n",
    "\n",
    "if doPlots and neuronType==2:\n",
    "    plt.figure(figsize=(7,3))\n",
    "    \n",
    "    # Compare correct classification traces for when inhibitory weights are set to 0 to when excitatory weights are set to 0.\n",
    "    plt.subplot(1,2,1)    \n",
    "\n",
    "\n",
    "    # Plot corrClass for when all excitatory neurons are set to 0\n",
    "    #     a1 = np.mean(corrClass_exc0, axis=1)*100 - a0\n",
    "    #     s1 = np.std(corrClass_exc0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])    \n",
    "    a1 = np.mean(corrClass_allExc0, axis=1)*100\n",
    "    s1 = np.std(corrClass_allExc0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])    \n",
    "\n",
    "    plt.fill_between(time_aligned_stim, a1-s1, a1+s1, alpha=0.3, edgecolor='m', facecolor='m')\n",
    "    plt.plot(time_aligned_stim, a1, 'm', label = 'all exc w set to 0')\n",
    "    plt.xlabel('Time since stimulus onset (ms)')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot corrClass for when excitatory neurons are set to 0\n",
    "    a = np.mean(corrClass_exc0, axis=1)*100 \n",
    "    s = np.std(corrClass_exc0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])    \n",
    "\n",
    "    plt.fill_between(time_aligned_stim, a-s, a+s, alpha=0.5, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, a, 'r', label = 'exc w set to 0')\n",
    "    plt.xlabel('Time since stimulus onset (ms)')\n",
    "    plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "\n",
    "\n",
    "    # Plot corrClass for when inhibitory neurons are set to 0\n",
    "    #     a = corrClass_inh0*100 \n",
    "    #     s = corrClass_inh0_std*100 /np.sqrt(corrClass.shape[1])\n",
    "    a = np.mean(corrClass_inh0, axis=1)*100 \n",
    "    s = np.std(corrClass_inh0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])  \n",
    "    plt.fill_between(time_aligned_stim, a-s, a+s, alpha=0.5, edgecolor='b', facecolor='b')\n",
    "    plt.plot(time_aligned_stim, a, 'b', label = 'inh w set to 0')\n",
    "\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, .7))\n",
    "\n",
    "\n",
    "    # Plot original corrClass\n",
    "    a = np.mean(corrClass, axis=1)*100 \n",
    "    s = np.std(corrClass, axis=1)*100 /np.sqrt(numTrials);\n",
    "    #     plt.figure()\n",
    "    #     plt.subplot(1,2,1)\n",
    "    #     plt.fill_between(time_aligned_stim, a-s, a+s, alpha=0.5, edgecolor='g', facecolor='g')\n",
    "    plt.plot(time_aligned_stim, a, 'g', label='original')\n",
    "    plt.xlabel('time since stimulus onset (ms)')\n",
    "    plt.ylabel('classification accuracy (%)')\n",
    "\n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    plt.subplot(1,2,2)        \n",
    "    \n",
    "    # Plot corrClass for all weights    \n",
    "    a0 = np.mean(corrClass, axis=1)*100 \n",
    "    '''\n",
    "    s0 = np.std(corrClass, axis=1)*100 /np.sqrt(numTrials);\n",
    "#     plt.figure()\n",
    "#     plt.subplot(1,2,1)\n",
    "    plt.fill_between(time_aligned_stim, a0-s0, a0+s0, alpha=0.3, edgecolor='g', facecolor='g')\n",
    "    plt.plot(time_aligned_stim, a0, 'g', label = 'original')\n",
    "    plt.xlabel('time since stimulus onset (ms)')\n",
    "    plt.ylabel('classification accuracy (%)')\n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "    '''        \n",
    "    plt.plot(time_aligned_stim, np.zeros((np.shape(time_aligned_stim))), 'g') \n",
    "    \n",
    "    \n",
    "    # Plot corrClass for when all excitatory neurons are set to 0\n",
    "#     a1 = np.mean(corrClass_exc0, axis=1)*100 - a0\n",
    "#     s1 = np.std(corrClass_exc0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])    \n",
    "    a1 = np.mean(corrClass_allExc0, axis=1)*100 - a0\n",
    "    s1 = np.std(corrClass_allExc0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])    \n",
    "    \n",
    "    plt.fill_between(time_aligned_stim, a1-s1, a1+s1, alpha=0.3, edgecolor='m', facecolor='m')\n",
    "    plt.plot(time_aligned_stim, a1, 'm', label = 'all exc w set to 0')\n",
    "    plt.xlabel('Time since stimulus onset (ms)')\n",
    "#     plt.ylabel('Classification accuracy (%)')    \n",
    "#     plt.ylabel('Change in classification accuracy (%)')    \n",
    "#     plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "    \n",
    "    \n",
    "    # Plot corrClass for when n excitatory neurons are set to 0\n",
    "    a1 = np.mean(corrClass_exc0, axis=1)*100 - a0\n",
    "    s1 = np.std(corrClass_exc0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])    \n",
    "    \n",
    "    plt.fill_between(time_aligned_stim, a1-s1, a1+s1, alpha=0.3, edgecolor='r', facecolor='r')\n",
    "    plt.plot(time_aligned_stim, a1, 'r', label = 'n exc w set to 0')\n",
    "    plt.xlabel('Time since stimulus onset (ms)')\n",
    "#     plt.ylabel('Classification accuracy (%)')    \n",
    "    plt.ylabel('Change in classification accuracy (%)')    \n",
    "    plt.xlim([time_aligned_stim[0], time_aligned_stim[-1]])\n",
    "    \n",
    "    \n",
    "    # Plot corrClass for when inhibitory neurons are set to 0\n",
    "#     a = corrClass_inh0*100 \n",
    "#     s = corrClass_inh0_std*100 /np.sqrt(corrClass.shape[1])\n",
    "    a2 = np.mean(corrClass_inh0, axis=1)*100 - a0\n",
    "    s2 = np.std(corrClass_inh0, axis=1)*100 #/np.sqrt(corrClass_exc0.shape[1])  \n",
    "    plt.fill_between(time_aligned_stim, a2-s2, a2+s2, alpha=0.3, edgecolor='b', facecolor='b')\n",
    "    plt.plot(time_aligned_stim, a2, 'b', label = 'inh w set to 0')\n",
    "    \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, .7))\n",
    "#     plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excitatory and Inhibitory Neurons Relative Contribution to the decoder\n",
    "\n",
    "We quantify the contribution of excitatory and inhibitory neurons to the encoding of the choice by measuring participation percentage, defined as the percentatge of a given population of neurons that has non-zero weights. We produce paraticipation curves, participation ratio at different values of svm regularizer (c), for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function finds the SVM decoder that predicts choices given responses in X by \n",
    "# using different values for c. At each value of c, it computes fraction of non-zero weights\n",
    "# for exc and inh neurons, separately (perActive_inh, perActive_exc). Also it computes the \n",
    "# classification error (perClassEr) at each value of c. \n",
    "# Outputs: perActive_inh, perActive_exc, perClassEr, cvect_\n",
    "\n",
    "def inh_exc_classContribution(X, Y, isinh): \n",
    "    import numpy as np\n",
    "    from sklearn import svm\n",
    "    \n",
    "    def perClassError(Y, Yhat):\n",
    "        import numpy as np\n",
    "        perClassEr = sum(abs(np.squeeze(Yhat).astype(float)-np.squeeze(Y).astype(float)))/len(Y)*100\n",
    "        return perClassEr\n",
    "    \n",
    "    Y = np.squeeze(Y); # class labels\n",
    "    \n",
    "    eps = 10**-10 # tiny number below which weight is considered 0\n",
    "    isinh = isinh>0;  # vector of size number of neurons (1: neuron is inhibitory, 0: neuron is excitatoey); here I am making sure to convert it to logical\n",
    "    n_inh = sum(isinh);\n",
    "    n_exc = sum(~ isinh);\n",
    "    cvect_ = 10**(np.arange(-4, 6,0.1))/len(Y);\n",
    "    perClassEr = [];\n",
    "    perActive_inh = [];\n",
    "    perActive_exc = [];\n",
    "    for i in range(len(cvect_)): # At each value of cvect we compute the fraction of non-zero weights for excit and inhibit neurons.\n",
    "        linear_svm = [];\n",
    "        linear_svm = svm.LinearSVC(C = cvect_[i], loss='squared_hinge', penalty='l1', dual=False);\n",
    "        linear_svm.fit(X, Y);\n",
    "        w = np.squeeze(linear_svm.coef_);\n",
    "        \n",
    "        perActive_inh.append(sum(abs(w[isinh])>eps)/ (n_inh + 0.) * 100.)\n",
    "        perActive_exc.append(sum(abs(w[~isinh])>eps)/ (n_exc + 0.) * 100.)\n",
    "        \n",
    "        perClassEr.append(perClassError(Y, linear_svm.predict(X)));\n",
    "    \n",
    "    return perActive_inh, perActive_exc, perClassEr, cvect_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if neuronType==2:\n",
    "    perActive_inh, perActive_exc, perClassEr, cvect_ = inh_exc_classContribution(X[:, ~np.isnan(inhRois)], Y, inhRois[~np.isnan(inhRois)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if doPlots and neuronType==2:    \n",
    "    plt.figure\n",
    "    plt.subplot(221)\n",
    "    plt.plot(cvect_, perActive_exc, 'b-', label = 'excit % non-zero w')\n",
    "    plt.plot(cvect_, perActive_inh, 'r-', label = 'inhibit % non-zero w')\n",
    "    plt.plot(cvect_, perClassEr, 'k-', label = '% classification error')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('c (inverse of regularization parameter)')\n",
    "    plt.ylim([-10,110])\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, .7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Another version of the analysis:\n",
    " We control for the different numbers of excitatory and inhibitory neurons by subsampling n excitatory neurons, where n is equal to the number of inhibitory neurons. More specifically, instead of sending the entire X to the function inh_exc_classContribution, we use a subset of X that includes equal number of exc and inh neurons (Exc neurons are randomly selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# equal number of exc and inh neurons\n",
    "if neuronType==2 and np.sum(w)!=0:\n",
    "    X_ = X[:, ~np.isnan(inhRois)];\n",
    "    inhRois_ = inhRois[~np.isnan(inhRois)].astype('int32')\n",
    "    ix_i = np.argwhere(inhRois_).squeeze()\n",
    "    ix_e = np.argwhere(inhRois_-1).squeeze()\n",
    "    n = len(ix_i);\n",
    "    Xei = np.zeros((len(Y), 2*n));\n",
    "    inhRois_ei = np.zeros((2*n));\n",
    "    perActive_inh = [];\n",
    "    perActive_exc = [];\n",
    "    perClassEr = [];\n",
    "    for i in range(numSamples):\n",
    "        msk = rng.permutation(ix_e)[0:n];\n",
    "        Xei[:, 0:n] = X_[:, msk]; # first n columns are X of random excit neurons.\n",
    "        inhRois_ei[0:n] = 0;\n",
    "\n",
    "        Xei[:, n:2*n] = X_[:, ix_i]; # second n icolumns are X of inhibit neurons.\n",
    "        inhRois_ei[n:2*n] = 1;\n",
    "        ai, ae, ce, cvect_ = inh_exc_classContribution(Xei, Y, inhRois_ei); # ai is of length cvect defined in inh_exc_classContribution\n",
    "        perActive_inh.append(ai); # numSamples x length(cvect_)\n",
    "        perActive_exc.append(ae);\n",
    "        perClassEr.append(ce);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if neuronType==2 and np.sum(w)!=0:\n",
    "    \n",
    "    # p value of comparing exc and inh non-zero weights pooled across values of c :\n",
    "    aa = np.array(perActive_exc).flatten()\n",
    "#     aa = aa[np.logical_and(aa>0 , aa<100)]\n",
    "    # np.shape(aa)\n",
    "\n",
    "    bb = np.array(perActive_inh).flatten()\n",
    "#     bb = bb[np.logical_and(bb>0 , bb<100)]\n",
    "    # np.shape(bb)\n",
    "\n",
    "    h, p_two = stats.ttest_ind(aa, bb)\n",
    "    p_tl = ttest2(aa, bb, tail='left')\n",
    "    p_tr = ttest2(aa, bb, tail='right')\n",
    "    print '\\np value (pooled for all values of c):\\nexc ~= inh : %.2f\\nexc < inh : %.2f\\nexc > inh : %.2f' %(p_two, p_tl, p_tr)\n",
    "\n",
    "\n",
    "    # two-tailed p-value\n",
    "    h, p_two = stats.ttest_ind(np.array(perActive_exc), np.array(perActive_inh))\n",
    "    # left-tailed p-value : excit < inhibit\n",
    "    p_tl = ttest2(np.array(perActive_exc), np.array(perActive_inh), tail='left')\n",
    "    # right-tailed p-value : excit > inhibit\n",
    "    p_tr = ttest2(np.array(perActive_exc), np.array(perActive_inh), tail='right')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot the c path :\n",
    "    if doPlots:\n",
    "        plt.figure() \n",
    "        plt.errorbar(cvect_, np.mean(np.array(perActive_exc), axis=0), yerr=2*np.std(np.array(perActive_exc), axis=0), color = 'b', label = 'excit % non-zero weights')\n",
    "        plt.errorbar(cvect_, np.mean(np.array(perActive_inh), axis=0), yerr=2*np.std(np.array(perActive_inh), axis=0), color = 'r', label = 'inhibit % non-zero weights')\n",
    "        plt.xscale('log')\n",
    "        plt.ylim([-10,110])\n",
    "        # plt.ylabel('% non-zero weights')\n",
    "        # plt.legend(loc='center left', bbox_to_anchor=(1, .7))\n",
    "        plt.xlim([cvect_[0], cvect_[-1]])\n",
    "\n",
    "        # plt.plot(cvect_, p_two, label = 'excit ~= inhibit')\n",
    "        # plt.plot(cvect_, p_tl, label = 'excit < inhibit')\n",
    "        # plt.plot(cvect_, p_tr, label = 'inhibit < excit')\n",
    "\n",
    "\n",
    "        # plt.figure()\n",
    "        plt.errorbar(cvect_ ,np.mean(np.array(perClassEr), axis=0), yerr=2*np.std(np.array(perClassEr), axis=0), color = 'k', label = 'classification error') # range(len(perClassEr[0]))\n",
    "        plt.ylim([-10,110])\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('c (inverse of regularization parameter)')\n",
    "        # plt.ylabel('classification error')\n",
    "        plt.xlim([cvect_[0], cvect_[-1]])\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, .7))\n",
    "        # plt.tight_layout()\n",
    "\n",
    "\n",
    "        plt.figure()\n",
    "        plt.subplot(212)\n",
    "        # plt.plot(cvect_, p_two<.05, label = 'excit ~= inhibit')\n",
    "        # plt.plot(cvect_, p_tl<.05, label = 'excit < inhibit')\n",
    "        # plt.plot(cvect_, p_tr<.05, label = 'inhibit < excit')\n",
    "        plt.plot(cvect_, p_two, label = 'excit ~= inhibit')\n",
    "        plt.plot(cvect_, p_tl, label = 'excit < inhibit')\n",
    "        plt.plot(cvect_, p_tr, label = 'inhibit < excit')\n",
    "        plt.xscale('log')\n",
    "        plt.ylim([-.1, 1.1])\n",
    "        # plt.legend(loc='center left', bbox_to_anchor=(1, .7))\n",
    "        plt.ylabel('P value')\n",
    "        plt.xlim([cvect_[0], cvect_[-1]])\n",
    "        plt.xlabel('c (inverse of regularization parameter)')\n",
    "\n",
    "        # plt.figure()\n",
    "        plt.subplot(211)\n",
    "        plt.plot(cvect_, p_two<.05, label = 'excit ~= inhibit')\n",
    "        plt.plot(cvect_, p_tl<.05, label = 'excit < inhibit')\n",
    "        plt.plot(cvect_, p_tr<.05, label = 'excit > inhibit')\n",
    "        plt.xscale('log')\n",
    "        plt.ylim([-.1, 1.1])\n",
    "        plt.xlim([cvect_[0], cvect_[-1]])\n",
    "        plt.ylabel('P value < .05')\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, .7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "83f30129-3af1-4c69-8229-e0e2afc95666"
    }
   },
   "source": [
    "## Save results as .mat files in a folder named svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "77f22c8d-89cf-40d7-b62b-9be2b9137cca"
    }
   },
   "outputs": [],
   "source": [
    "if trialHistAnalysis:\n",
    "#     ep_ms = np.round((ep-eventI)*frameLength)\n",
    "    th_stim_dur = []\n",
    "    svmn = 'svmPrevChoice_%sN_%sITIs_ep%d-%dms_r%d_' %(ntName, itiName, ep_ms[0], ep_ms[-1], roundi)\n",
    "else:\n",
    "    svmn = 'svmCurrChoice_%sN_ep%d-%dms_r%d_' %(ntName, ep_ms[0], ep_ms[-1], roundi)   \n",
    "print '\\n', svmn[:-1]\n",
    "\n",
    "if saveResults:\n",
    "    print 'Saving .mat file'\n",
    "    d = os.path.join(os.path.dirname(pnevFileName), 'svm')\n",
    "    if not os.path.exists(d):\n",
    "        print 'creating svm folder'\n",
    "        os.makedirs(d)\n",
    "\n",
    "    svmName = os.path.join(d, svmn+os.path.basename(pnevFileName))\n",
    "    print(svmName)\n",
    "    # scio.savemat(svmName, {'trsExcluded':trsExcluded, 'NsExcluded':NsExcluded, 'meanX':meanX, 'stdX':stdX, 'thAct':thAct, 'thTrsWithSpike':thTrsWithSpike, 'ep_ms':ep_ms, 'th_stim_dur':th_stim_dur})\n",
    "    if neuronType==2:\n",
    "#         scio.savemat(svmName, {'thAct':thAct, 'thTrsWithSpike':thTrsWithSpike, 'ep_ms':ep_ms, 'th_stim_dur':th_stim_dur, 'numSamples':numSamples, 'trsExcluded':trsExcluded, 'NsExcluded':NsExcluded, 'NsRand':NsRand, 'meanX':meanX, 'stdX':stdX, 'w':w, 'b':b, 'cbest':cbest, 'corrClass':corrClass, 'perClassErrorTrain_data':perClassErrorTrain_data, 'perClassErrorTrain_shfl':perClassErrorTrain_shfl, 'perClassErrorTest_data':perClassErrorTest_data, 'perClassErrorTest_shfl':perClassErrorTest_shfl, 'perClassErrorTest':perClassErrorTest, 'perClassErrorTrain':perClassErrorTrain, 'cvect':cvect, 'perActive_inh':perActive_inh, 'perActive_exc':perActive_exc, 'perClassEr':perClassEr, 'cvect_':cvect_, 'trainE':trainE, 'train_err_exc0':train_err_exc0, 'train_err_inh0':train_err_inh0, 'corrClass_exc0':corrClass_exc0, 'corrClass_inh0':corrClass_inh0, 'train_err_allExc0':train_err_allExc0, 'corrClass_allExc0':corrClass_allExc0})\n",
    "        scio.savemat(svmName, {'thAct':thAct, 'thTrsWithSpike':thTrsWithSpike, 'ep_ms':ep_ms, 'th_stim_dur':th_stim_dur, 'numSamples':numSamples, 'trsExcluded':trsExcluded, 'NsExcluded':NsExcluded, 'NsRand':NsRand, 'meanX':meanX, 'stdX':stdX, 'w':w, 'b':b, 'cbest':cbest, 'corrClass':corrClass, 'perClassErrorTrain_data':perClassErrorTrain_data, 'perClassErrorTrain_shfl':perClassErrorTrain_shfl, 'perClassErrorTest_data':perClassErrorTest_data, 'perClassErrorTest_shfl':perClassErrorTest_shfl, 'perClassErrorTest':perClassErrorTest, 'perClassErrorTrain':perClassErrorTrain, 'cvect':cvect, 'perActive_inh':perActive_inh, 'perActive_exc':perActive_exc, 'perClassEr':perClassEr, 'cvect_':cvect_, 'trainE':trainE, 'train_err_exc0':train_err_exc0, 'train_err_inh0':train_err_inh0, 'corrClass_exc0':corrClass_exc0, 'corrClass_inh0':corrClass_inh0, 'train_err_allExc0':train_err_allExc0, 'corrClass_allExc0':corrClass_allExc0, 'perClassErrorTrain_data_inh':perClassErrorTrain_data_inh, 'perClassErrorTest_data_inh':perClassErrorTest_data_inh, 'perClassErrorTrain_shfl_inh':perClassErrorTrain_shfl_inh, 'perClassErrorTest_shfl_inh':perClassErrorTest_shfl_inh, 'perClassErrorTrain_data_allExc':perClassErrorTrain_data_allExc, 'perClassErrorTest_data_allExc':perClassErrorTest_data_allExc, 'perClassErrorTrain_shfl_allExc':perClassErrorTrain_shfl_allExc, 'perClassErrorTest_shfl_allExc':perClassErrorTest_shfl_allExc, 'perClassErrorTrain_data_exc':perClassErrorTrain_data_exc, 'perClassErrorTest_data_exc':perClassErrorTest_data_exc, 'perClassErrorTrain_shfl_exc':perClassErrorTrain_shfl_exc, 'perClassErrorTest_shfl_exc':perClassErrorTest_shfl_exc, 'corrClass_allExc':corrClass_allExc, 'corrClass_exc':corrClass_exc, 'corrClass_inh':corrClass_inh}) \n",
    "    else:\n",
    "        scio.savemat(svmName, {'thAct':thAct, 'thTrsWithSpike':thTrsWithSpike, 'ep_ms':ep_ms, 'th_stim_dur':th_stim_dur, 'numSamples':numSamples, 'trsExcluded':trsExcluded, 'NsExcluded':NsExcluded, 'NsRand':NsRand, 'meanX':meanX, 'stdX':stdX, 'w':w, 'b':b, 'cbest':cbest, 'corrClass':corrClass, 'perClassErrorTrain_data':perClassErrorTrain_data, 'perClassErrorTrain_shfl':perClassErrorTrain_shfl, 'perClassErrorTest_data':perClassErrorTest_data, 'perClassErrorTest_shfl':perClassErrorTest_shfl, 'perClassErrorTest':perClassErrorTest, 'perClassErrorTrain':perClassErrorTrain, 'cvect':cvect})\n",
    "\n",
    "    # save normalized traces as well                       \n",
    "    # scio.savemat(svmName, {w':w, 'b':b, 'cbest':cbest, 'corrClass':corrClass, 'trsExcluded':trsExcluded, 'NsExcluded':NsExcluded, 'meanX':meanX, 'stdX':stdX, 'X':X, 'Y':Y, 'Xt':Xt, 'Xtg':Xtg, 'Xtc':Xtc, 'Xtr':Xtr, 'Xtp':Xtp})\n",
    "    # 'linear_svm':linear_svm, \n",
    "\n",
    "    # append : doesn't quite work\n",
    "    # if os.path.isfile(svmName): \n",
    "    #     with open(svmName,'ab') as f:\n",
    "    #         sci.io.savemat(f, {'perClassErrorTrain_data':perClassErrorTrain_data, 'perClassErrorTrain_shfl':perClassErrorTrain_shfl, 'perClassErrorTest_data':perClassErrorTest_data, 'perClassErrorTest_shfl':perClassErrorTest_shfl}) # append\n",
    "    # else:\n",
    "else:\n",
    "    print 'Not saving .mat file'\n",
    "    \n",
    "    \n",
    "# If you want to save the linear_svm objects as mat file, you need to take care of \n",
    "# None values and set to them []:\n",
    "\n",
    "# import inspect\n",
    "# inspect.getmembers(summary_shfl_exc[0])\n",
    "\n",
    "# for i in range(np.shape(summary_shfl_exc)[0]):\n",
    "#     summary_shfl_exc[i].model.random_state = []\n",
    "#     summary_shfl_exc[i].model.class_weight = []\n",
    "# [summary_shfl_exc[i].model.random_state for i in range(np.shape(summary_shfl_exc)[0])]\n",
    "# [summary_shfl_exc[i].model.class_weight for i in range(np.shape(summary_shfl_exc)[0])]\n",
    "\n",
    "# scio.savemat(svmName, {'summary_shfl_exc':summary_shfl_exc})\n",
    "\n",
    "# Data = scio.loadmat(svmName, variable_names=['summary_shfl_exc'] ,squeeze_me=True,struct_as_record=False)\n",
    "# summary_shfl_exc = Data.pop('summary_shfl_exc')\n",
    "# summary_shfl_exc[0].model.intercept_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move the autosaved .html file to a folder named \"figs\"\n",
    "    Notebook html file will be autosaved in the notebook directory if jupyter_notebook_config.py exists in ~/.jupyter and includes the function script_post_save. Below we move the html to a directory named figs inside the root directory which contains moreFile, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure autosave is done so you move the most recent html file to figs directory.\n",
    "if 'ipykernel' in sys.modules and saveHTML:\n",
    "    %autosave 1    \n",
    "    # import time    \n",
    "    # time.sleep(2) \n",
    "    \n",
    "    d = os.path.join(os.path.dirname(pnevFileName),'figs')\n",
    "    if not os.path.exists(d):\n",
    "        print 'creating figs folder'\n",
    "        os.makedirs(d)\n",
    "\n",
    "    htmlName = os.path.join(d, svmn[:-1]+'.html')\n",
    "    print htmlName\n",
    "    import shutil\n",
    "    shutil.move(os.path.join(os.getcwd(), 'mainSVM_notebook.html'), htmlName)\n",
    "\n",
    "    # go back to default autosave interval\n",
    "    %autosave 120 "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
